MetaArgs: null
Stages:
- BaseName: python:3.5
  Commands:
  - Env:
    - Key: PYTHONHASHSEED
      Value: "0"
    Name: env
  - Env:
    - Key: PYTHONIOENCODING
      Value: UTF-8
    Name: env
  - Env:
    - Key: PIP_DISABLE_PIP_VERSION_CHECK
      Value: "1"
    Name: env
  - Key: JAVA_MAJOR_VERSION
    Name: arg
    Value: "8"
  - Key: JAVA_UPDATE_VERSION
    Name: arg
    Value: "112"
  - Key: JAVA_BUILD_NUMBER
    Name: arg
    Value: "15"
  - Env:
    - Key: JAVA_HOME
      Value: /usr/jdk1.${JAVA_MAJOR_VERSION}.0_${JAVA_UPDATE_VERSION}
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:$JAVA_HOME/bin
    Name: env
  - CmdLine:
    - 'curl -sL --retry 3 --insecure   --header "Cookie: oraclelicense=accept-securebackup-cookie;"   "http://download.oracle.com/otn-pub/java/jdk/${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-b${JAVA_BUILD_NUMBER}/server-jre-${JAVA_MAJOR_VERSION}u${JAVA_UPDATE_VERSION}-linux-x64.tar.gz"   |
      gunzip   | tar x -C /usr/   && ln -s $JAVA_HOME /usr/java   && rm -rf $JAVA_HOME/man'
    Name: run
    PrependShell: true
  - Env:
    - Key: HADOOP_VERSION
      Value: 2.7.3
    Name: env
  - Env:
    - Key: HADOOP_HOME
      Value: /usr/hadoop-$HADOOP_VERSION
    Name: env
  - Env:
    - Key: HADOOP_CONF_DIR
      Value: $HADOOP_HOME/etc/hadoop
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:$HADOOP_HOME/bin
    Name: env
  - CmdLine:
    - curl -sL --retry 3   "http://archive.apache.org/dist/hadoop/common/hadoop-$HADOOP_VERSION/hadoop-$HADOOP_VERSION.tar.gz"   |
      gunzip   | tar -x -C /usr/  && rm -rf $HADOOP_HOME/share/doc  && chown -R root:root
      $HADOOP_HOME
    Name: run
    PrependShell: true
  - Env:
    - Key: SPARK_VERSION
      Value: 2.1.1
    Name: env
  - Env:
    - Key: SPARK_PACKAGE
      Value: spark-${SPARK_VERSION}-bin-without-hadoop
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /usr/spark-${SPARK_VERSION}
    Name: env
  - Env:
    - Key: SPARK_DIST_CLASSPATH
      Value: '"$HADOOP_HOME/etc/hadoop/*:$HADOOP_HOME/share/hadoop/common/lib/*:$HADOOP_HOME/share/hadoop/common/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/hdfs/lib/*:$HADOOP_HOME/share/hadoop/hdfs/*:$HADOOP_HOME/share/hadoop/yarn/lib/*:$HADOOP_HOME/share/hadoop/yarn/*:$HADOOP_HOME/share/hadoop/mapreduce/lib/*:$HADOOP_HOME/share/hadoop/mapreduce/*:$HADOOP_HOME/share/hadoop/tools/lib/*"'
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:${SPARK_HOME}/bin
    Name: env
  - CmdLine:
    - curl -sL --retry 3   "http://d3kbcqa49mib13.cloudfront.net/${SPARK_PACKAGE}.tgz"   |
      gunzip   | tar x -C /usr/  && mv /usr/$SPARK_PACKAGE $SPARK_HOME  && chown -R
      root:root $SPARK_HOME
    Name: run
    PrependShell: true
  - CmdLine:
    - apt-get update --fix-missing && apt-get install -y wget bzip2 ca-certificates     libglib2.0-0
      libxext6 libsm6 libxrender1     git mercurial subversion
    Name: run
    PrependShell: true
  - CmdLine:
    - echo 'export PATH=/opt/conda/bin:$PATH' > /etc/profile.d/conda.sh &&     wget
      --quiet https://repo.continuum.io/archive/Anaconda3-4.3.1-Linux-x86_64.sh -O
      ~/anaconda.sh &&     /bin/bash ~/anaconda.sh -b -p /opt/conda &&     rm ~/anaconda.sh
    Name: run
    PrependShell: true
  - CmdLine:
    - apt-get install -y curl grep sed dpkg &&     TINI_VERSION=`curl https://github.com/krallin/tini/releases/latest
      | grep -o "/v.*\"" | sed 's:^..\(.*\).$:\1:'` &&     curl -L "https://github.com/krallin/tini/releases/download/v${TINI_VERSION}/tini_${TINI_VERSION}.deb"
      > tini.deb &&     dpkg -i tini.deb &&     rm tini.deb &&     apt-get clean
    Name: run
    PrependShell: true
  - Env:
    - Key: PATH
      Value: /opt/conda/bin:$PATH
    Name: env
  - CmdLine:
    - apt-get update --fix-missing && apt-get install -y gfortran libatlas-base-dev
      gfortran pkg-config             libfreetype6-dev libxft-dev libpng-dev libhdf5-serial-dev
      g++             make patch lib32ncurses5-dev
    Name: run
    PrependShell: true
  - Name: user
    User: root
  - CmdLine:
    - conda install -y gcc
    Name: run
    PrependShell: true
  - Env:
    - Key: GRAPH_FRAMES_VERSION
      Value: 0.4.0-spark2.1-s_2.11
    Name: env
  - Chown: ""
    Name: add
    SourcesAndDest:
    - ./graphframes-dist/graphframes-${GRAPH_FRAMES_VERSION}
    - $SPARK_HOME/graphframes
  - CmdLine:
    - cd $SPARK_HOME/graphframes &&     ./build/sbt "set test in assembly := {}" clean
      assembly &&     mv $SPARK_HOME/graphframes/python/graphframes $SPARK_HOME/python/pyspark
    Name: run
    PrependShell: true
  - Env:
    - Key: MONGO_SPARK_CONNECTOR_VERSION
      Value: 2.0.0
    Name: env
  - Env:
    - Key: MONGO_SPARK_VERSION
      Value: "2.10"
    Name: env
  - Chown: ""
    Name: add
    SourcesAndDest:
    - ./mongo-spark-connector/mongo-spark-connector_${MONGO_SPARK_VERSION}_${MONGO_SPARK_CONNECTOR_VERSION}
    - $SPARK_HOME/org.mongodb.spark
  - CmdLine:
    - cd $SPARK_HOME/org.mongodb.spark &&     ./sbt "set test in assembly := {}" clean
      assembly
    Name: run
    PrependShell: true
  - CmdLine:
    - cd /opt &&   git clone --recursive https://github.com/dmlc/xgboost &&   cd xgboost
      &&   make -j4
    Name: run
    PrependShell: true
  - Env:
    - Key: PYTHONPATH
      Value: /opt/xgboost/python-package
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: $SPARK_HOME/python/lib/py4j-0.10.4-src.zip:$SPARK_HOME/python/:$PYTHONPATH
    Name: env
  - Name: user
    User: $NB_USER
  - CmdLine:
    - pip install spark-sklearn
    Name: run
    PrependShell: true
  - Env:
    - Key: PYSPARK_SUBMIT_ARGS
      Value: '"--packages graphframes:graphframes:${GRAPH_FRAMES_VERSION},org.mongodb.spark:mongo-spark-connector_${MONGO_SPARK_VERSION}:${MONGO_SPARK_CONNECTOR_VERSION}
        pyspark-shell"'
    Name: env
  - Name: workdir
    Path: $SPARK_HOME
  - CmdLine:
    - bin/spark-class
    - org.apache.spark.deploy.master.Master
    Name: cmd
    PrependShell: false
  From:
    Image: python:3.5
  Name: ""
  Platform: ""
  SourceCode: FROM python:3.5
