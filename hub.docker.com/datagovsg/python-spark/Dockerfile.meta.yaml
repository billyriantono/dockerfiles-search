MetaArgs: null
Stages:
- BaseName: python:2.7
  Commands:
  - CmdLine:
    - set -x &&     apt-get update &&     apt-get install --no-install-recommends
      -y software-properties-common &&     echo "deb http://ppa.launchpad.net/webupd8team/java/ubuntu
      xenial main"  >         /etc/apt/sources.list.d/webupd8team-java.list &&     echo
      "deb-src http://ppa.launchpad.net/webupd8team/java/ubuntu xenial main" >>         /etc/apt/sources.list.d/webupd8team-java.list
      &&     apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv-keys EEA14886
      &&     echo oracle-java8-installer shared/accepted-oracle-license-v1-1 select
      true | /usr/bin/debconf-set-selections &&     apt-get update && echo yes | apt-get
      install -y --force-yes oracle-java8-installer &&     apt-get update && apt-get
      install oracle-java8-set-default &&     apt-get remove software-properties-common
      -y --auto-remove &&     apt-get clean &&     rm -rf /var/lib/apt/lists/* /tmp/*
      /var/tmp/*
    Name: run
    PrependShell: true
  - Env:
    - Key: JAVA_HOME
      Value: /usr/lib/jvm/java-8-oracle
    Name: env
  - Key: HADOOP_VERSION
    Name: arg
    Value: 2.6.1
  - Key: SPARK_VERSION
    Name: arg
    Value: 1.6.1
  - Env:
    - Key: HADOOP_HOME
      Value: /opt/hadoop
    Name: env
  - Env:
    - Key: PATH
      Value: ${PATH}:${HADOOP_HOME}/bin:${HADOOP_HOME}/sbin
    Name: env
  - Env:
    - Key: HADOOP_MAPRED_HOME
      Value: ${HADOOP_HOME}
    Name: env
  - Env:
    - Key: HADOOP_COMMON_HOME
      Value: ${HADOOP_HOME}
    Name: env
  - Env:
    - Key: HADOOP_HDFS_HOME
      Value: ${HADOOP_HOME}
    Name: env
  - Env:
    - Key: YARN_HOME
      Value: ${HADOOP_HOME}
    Name: env
  - Env:
    - Key: HADOOP_COMMON_LIB_NATIVE_DIR
      Value: ${HADOOP_HOME}/lib/native
    Name: env
  - Env:
    - Key: HADOOP_OPTS
      Value: '"-Djava.library.path=${HADOOP_HOME}/lib"'
    Name: env
  - Env:
    - Key: HDFS_CONF_DIR
      Value: ${HADOOP_HOME}/etc/hadoop
    Name: env
  - Env:
    - Key: YARN_CONF_DIR
      Value: ${HADOOP_HOME}/etc/hadoop
    Name: env
  - Env:
    - Key: HADOOP_CONF_DIR
      Value: ${HADOOP_HOME}/etc/hadoop
    Name: env
  - Env:
    - Key: HIVE_CONF_DIR
      Value: ${HADOOP_CONF_DIR}
    Name: env
  - Env:
    - Key: SPARK_16_HOME
      Value: /opt/spark-${SPARK_16_VERSION}
    Name: env
  - Env:
    - Key: SPARK_20_HOME
      Value: /opt/spark-${SPARK_20_VERSION}
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /opt/spark-${SPARK_VERSION}
    Name: env
  - Env:
    - Key: PYSPARK_PYTHON
      Value: python
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:${SPARK_HOME}/bin
    Name: env
  - CmdLine:
    - if [ "${SPARK_VERSION}" = "${SPARK_16_HOME}" ]; then     export PYSPARK_SUBMIT_ARGS="--packages
      com.databricks:spark-csv_2.10:1.5.0,com.databricks:spark-avro_2.10:2.0.1,graphframes:graphframes:0.1.0-spark1.6
      pyspark-shell";     export PYTHONPATH=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.9-src.zip;   elif
      [ "${SPARK_VERSION}" = "${SPARK_20_HOME}" ]; then     export PYSPARK_SUBMIT_ARGS="--packages
      com.databricks:spark-csv_2.11:1.5.0,com.databricks:spark-avro_2.11:3.1.0,graphframes:graphframes:0.3.0-spark2.0-s_2.11
      pyspark-shell";     export PYTHONPATH=${SPARK_HOME}/python:${SPARK_HOME}/python/lib/py4j-0.10.3-src.zip;   fi
    Name: run
    PrependShell: true
  - Env:
    - Key: SPARK_MASTER_OPTS
      Value: '"-Dspark.driver.port=7001 -Dspark.fileserver.port=7002 -Dspark.broadcast.port=7003
        -Dspark.replClassServer.port=7004 -Dspark.blockManager.port=7005 -Dspark.executor.port=7006
        -Dspark.ui.port=4040 -Dspark.broadcast.factory=org.apache.spark.broadcast.HttpBroadcastFactory"'
    Name: env
  - Env:
    - Key: SPARK_WORKER_OPTS
      Value: '"-Dspark.driver.port=7001 -Dspark.fileserver.port=7002 -Dspark.broadcast.port=7003
        -Dspark.replClassServer.port=7004 -Dspark.blockManager.port=7005 -Dspark.executor.port=7006
        -Dspark.ui.port=4040 -Dspark.broadcast.factory=org.apache.spark.broadcast.HttpBroadcastFactory"'
    Name: env
  - Env:
    - Key: SPARK_MASTER_PORT
      Value: "7077"
    Name: env
  - Env:
    - Key: SPARK_MASTER_WEBUI_PORT
      Value: "8080"
    Name: env
  - Env:
    - Key: SPARK_WORKER_PORT
      Value: "8888"
    Name: env
  - Env:
    - Key: SPARK_WORKER_WEBUI_PORT
      Value: "8081"
    Name: env
  - Env:
    - Key: SQOOP_HOME
      Value: /opt/sqoop
    Name: env
  - Env:
    - Key: PATH
      Value: ${PATH}:${SQOOP_HOME}/bin:${HADOOP_HOME}/bin
    Name: env
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - install.sh
    - /install.sh
  - CmdLine:
    - /install.sh
    Name: run
    PrependShell: true
  - Name: expose
    Ports:
    - "4040"
    - "7001"
    - "7002"
    - "7003"
    - "7004"
    - "7005"
    - "7006"
    - "7077"
    - "8080"
    - "8081"
    - "8888"
  - CmdLine:
    - echo $PYSPARK_SUBMIT_ARGS
    Name: run
    PrependShell: true
  - CmdLine:
    - '''/bin/bash'''
    Name: cmd
    PrependShell: true
  From:
    Image: python:2.7
  Name: ""
  Platform: ""
  SourceCode: FROM python:2.7
