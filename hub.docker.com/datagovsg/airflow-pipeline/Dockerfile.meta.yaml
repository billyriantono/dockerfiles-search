MetaArgs: null
Stages:
- BaseName: datagovsg/python-spark-airflow:1.7
  Commands:
  - Maintainer: Chris Sng <chris@data.gov.sg>
    Name: maintainer
  - Env:
    - Key: GOSU_VERSION
      Value: "1.7"
    Name: env
  - CmdLine:
    - set -x     && apt-get update && apt-get install -y --no-install-recommends ca-certificates
      wget && rm -rf /var/lib/apt/lists/*     && wget -O /usr/local/bin/gosu "https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg
      --print-architecture)"     && wget -O /usr/local/bin/gosu.asc "https://github.com/tianon/gosu/releases/download/$GOSU_VERSION/gosu-$(dpkg
      --print-architecture).asc"     && export GNUPGHOME="$(mktemp -d)"     && gpg
      --keyserver ha.pool.sks-keyservers.net --recv-keys B42F6819007F00F88E364FD4036A9C25BF357DD4     &&
      gpg --batch --verify /usr/local/bin/gosu.asc /usr/local/bin/gosu     && rm -r
      "$GNUPGHOME" /usr/local/bin/gosu.asc     && chmod +x /usr/local/bin/gosu     &&
      gosu nobody true     && apt-get -y autoremove
    Name: run
    PrependShell: true
  - Env:
    - Key: USER
      Value: afpuser
    Name: env
  - Env:
    - Key: GROUP
      Value: hadoop
    Name: env
  - CmdLine:
    - groupadd -r "${GROUP}" && useradd -rmg "${GROUP}" "${USER}"
    Name: run
    PrependShell: true
  - Env:
    - Key: SCHEDULER_RUNS
      Value: "5"
    Name: env
  - Env:
    - Key: AIRFLOW_PARALLELISM
      Value: "8"
    Name: env
  - Env:
    - Key: AIRFLOW_DAG_CONCURRENCY
      Value: "6"
    Name: env
  - Env:
    - Key: POSTGRES_HOST
      Value: localhost
    Name: env
  - Env:
    - Key: POSTGRES_PORT
      Value: "5999"
    Name: env
  - Env:
    - Key: POSTGRES_USER
      Value: fixme
    Name: env
  - Env:
    - Key: POSTGRES_PASSWORD
      Value: fixme
    Name: env
  - Env:
    - Key: POSTGRES_DB
      Value: airflow
    Name: env
  - Env:
    - Key: PIPELINE_DATA_PATH
      Value: hdfs://dsg-cluster-node01:8020/datasets/"${GROUP}"
    Name: env
  - Name: workdir
    Path: ${AIRFLOW_HOME}
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - requirements.txt
    - ${AIRFLOW_HOME}/requirements.txt
  - CmdLine:
    - pip install -r "${AIRFLOW_HOME}/requirements.txt"
    Name: run
    PrependShell: true
  - Name: volume
    Volumes:
    - ${AIRFLOW_HOME}/logs
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - airflow.cfg
    - ${AIRFLOW_HOME}/airflow.cfg
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - hadoop-sample/conf/
    - ${HADOOP_CONF_DIR}/
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - dags/
    - ${AIRFLOW_DAG}
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - setup_auth.py
    - ${AIRFLOW_HOME}/setup_auth.py
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - install_spark_packages.py
    - ${AIRFLOW_HOME}/install_spark_packages.py
  - CmdLine:
    - gosu "${USER}" python install_spark_packages.py
    Name: run
    PrependShell: true
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - entrypoint.sh
    - ${AIRFLOW_HOME}/entrypoint.sh
  - CmdLine:
    - ./entrypoint.sh
    Name: entrypoint
    PrependShell: false
  From:
    Image: datagovsg/python-spark-airflow:1.7
  Name: ""
  Platform: ""
  SourceCode: FROM datagovsg/python-spark-airflow:1.7
