MetaArgs: null
Stages:
- BaseName: joegagliardo/ubuntu
  Commands:
  - Maintainer: joegagliardo
    Name: maintainer
  - Name: expose
    Ports:
    - "10000"
    - "10001"
    - "10002"
    - "22"
    - "3306"
    - "49707"
    - "50010"
    - "50020"
    - "50070"
    - "50075"
    - "50090"
    - "60000"
    - "60010"
    - "7077"
    - "7078"
    - "8030"
    - "8031"
    - "8032"
    - "8033"
    - "8040"
    - "8041"
    - "8042"
    - "8088"
    - "9042"
    - "9083"
    - "9160"
    - "9870"
  - Key: HIVEUSER_PASSWORD
    Name: arg
    Value: hivepassword
  - Key: HIVE_METASTORE
    Name: arg
    Value: hivemetastore
  - Key: HADOOP_VERSION
    Name: arg
    Value: 3.1.1
  - Key: HIVE_VERSION
    Name: arg
    Value: 3.1.0
  - Key: PIG_VERSION
    Name: arg
    Value: 0.17.0
  - Key: SPARK_VERSION
    Name: arg
    Value: 2.3.2
  - Key: ZOOKEEPER_VERSION
    Name: arg
    Value: 3.4.13
  - Key: SQOOP_VERSION
    Name: arg
    Value: 1.4.7
  - Key: FLUME_VERSION
    Name: arg
    Value: 1.8.0
  - Key: FLINK_VERSION
    Name: arg
    Value: 1.6.1
  - Key: HBASE_VERSION
    Name: arg
    Value: 2.1.0
  - Key: SPARK_CASSANDRA_VERSION
    Name: arg
    Value: 2.3.1-s_2.11
  - Key: MONGO_JAVA_DRIVER_VERSION
    Name: arg
    Value: 3.8.2
  - Key: MONGO_HADOOP_VERSION
    Name: arg
    Value: 2.0.2
  - Key: COCKROACH_VERSION
    Name: arg
    Value: 2.0.5
  - Key: POSTGRES_JDBC_VERSION
    Name: arg
    Value: 42.2.5
  - Key: CASSANDRA_VERSION
    Name: arg
    Value: "311"
  - Key: HIVE_HCATALOG_HBASE_STORAGE_HANDLER_VERSION
    Name: arg
    Value: 0.13.1
  - Key: HADOOP_BASE_URL
    Name: arg
    Value: http://mirrors.sonic.net/apache/hadoop/common
  - Key: HADOOP_FILE
    Name: arg
    Value: hadoop-${HADOOP_VERSION}.tar.gz
  - Key: HADOOP_URL
    Name: arg
    Value: ${HADOOP_BASE_URL}/hadoop-${HADOOP_VERSION}/${HADOOP_FILE}
  - Key: HADOOP_FOLDER
    Name: arg
    Value: hadoop-${HADOOP_VERSION}
  - Key: HIVE_BASE_URL
    Name: arg
    Value: http://apache.claz.org/hive
  - Key: HIVE_FILE
    Name: arg
    Value: apache-hive-${HIVE_VERSION}-bin.tar.gz
  - Key: HIVE_URL
    Name: arg
    Value: ${HIVE_BASE_URL}/hive-${HIVE_VERSION}/${HIVE_FILE}
  - Key: HIVE_FOLDER
    Name: arg
    Value: apache-hive-${HIVE_VERSION}-bin
  - Key: PIG_BASE_URL
    Name: arg
    Value: http://apache.claz.org/pig
  - Key: PIG_FILE
    Name: arg
    Value: pig-${PIG_VERSION}.tar.gz
  - Key: PIG_URL
    Name: arg
    Value: ${PIG_BASE_URL}/pig-${PIG_VERSION}/${PIG_FILE}
  - Key: PIG_FOLDER
    Name: arg
    Value: pig-${PIG_VERSION}
  - Key: SPARK_BASE_URL
    Name: arg
    Value: http://apache.claz.org/spark
  - Key: SPARK_FILE
    Name: arg
    Value: spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
  - Key: SPARK_URL
    Name: arg
    Value: ${SPARK_BASE_URL}/spark-${SPARK_VERSION}/${SPARK_FILE}
  - Key: SPARK_FOLDER
    Name: arg
    Value: spark-${SPARK_VERSION}-bin-hadoop2.7
  - Key: ZOOKEEPER_BASE_URL
    Name: arg
    Value: http://apache.claz.org/zookeeper
  - Key: ZOOKEEPER_FILE
    Name: arg
    Value: zookeeper-${ZOOKEEPER_VERSION}.tar.gz
  - Key: ZOOKEEPER_URL
    Name: arg
    Value: ${ZOOKEEPER_BASE_URL}/zookeeper-${ZOOKEEPER_VERSION}/${ZOOKEEPER_FILE}
  - Key: ZOOKEEPER_FOLDER
    Name: arg
    Value: zookeeper-${ZOOKEEPER_VERSION}
  - Key: SQOOP_BASE_URL
    Name: arg
    Value: http://apache.claz.org/sqoop/1.4.7
  - Key: SQOOP_FILE
    Name: arg
    Value: sqoop-${SQOOP_VERSION}.bin__hadoop-2.6.0.tar.gz
  - Key: SQOOP_URL
    Name: arg
    Value: ${SQOOP_BASE_URL}/${SQOOP_FILE}
  - Key: SQOOP_FOLDER
    Name: arg
    Value: sqoop-${SQOOP_VERSION}.bin__hadoop-2.6.0
  - Key: FLUME_BASE_URL
    Name: arg
    Value: http://apache.claz.org/flume
  - Key: FLUME_FILE
    Name: arg
    Value: apache-flume-${FLUME_VERSION}-bin.tar.gz
  - Key: FLUME_URL
    Name: arg
    Value: ${FLUME_BASE_URL}/${FLUME_VERSION}/${FLUME_FILE}
  - Key: FLUME_FOLDER
    Name: arg
    Value: apache-flume-${FLUME_VERSION}-bin
  - Key: FLINK_BASE_URL
    Name: arg
    Value: http://apache.claz.org/flink/flink-${FLINK_VERSION}/
  - Key: FLINK_FILE
    Name: arg
    Value: flink-${FLINK_VERSION}-bin-scala_2.11.tgz
  - Key: FLINK_URL
    Name: arg
    Value: ${FLINK_BASE_URL}/${FLINK_FILE}
  - Key: OOZIE_VERSION
    Name: arg
    Value: 5.0.0
  - Key: OOZIE_BASE_URL
    Name: arg
    Value: http://apache.claz.org/oozie/${OOZIE_VERSION}
  - Key: HBASE_BASE_URL
    Name: arg
    Value: http://apache.mirrors.pair.com/hbase
  - Key: HBASE_FILE
    Name: arg
    Value: hbase-${HBASE_VERSION}-bin.tar.gz
  - Key: HBASE_URL
    Name: arg
    Value: ${HBASE_BASE_URL}/${HBASE_VERSION}/${HBASE_FILE}
  - Key: HBASE_FOLDER
    Name: arg
    Value: hbase-${HBASE_VERSION}
  - Key: SPARK_CASSANDRA_BASE_URL
    Name: arg
    Value: http://dl.bintray.com/spark-packages/maven/datastax/spark-cassandra-connector
  - Key: SPARK_CASSANDRA_FILE
    Name: arg
    Value: spark-cassandra-connector-${SPARK_CASSANDRA_VERSION}.jar
  - Key: SPARK_CASSANDRA_URL
    Name: arg
    Value: ${SPARK_CASSANDRA_BASE_URL}/${SPARK_CASSANDRA_VERSION}/${SPARK_CASSANDRA_FILE}
  - Key: MONGO_JAVA_DRIVER_BASE_URL
    Name: arg
    Value: https://repo1.maven.org/maven2/org/mongodb
  - Key: MONGO_JAVA_DRIVER_FILE
    Name: arg
    Value: mongo-java-driver-${MONGO_JAVA_DRIVER_VERSION}.jar
  - Key: MONGO_JAVA_DRIVER_URL
    Name: arg
    Value: ${MONGO_JAVA_DRIVER_BASE_URL}/mongo-java-driver/${MONGO_JAVA_DRIVER_VERSION}/${MONGO_JAVA_DRIVER_FILE}
  - Key: MONGO_HADOOP_BASE_URL
    Name: arg
    Value: https://repo1.maven.org/maven2/org/mongodb/mongo-hadoop
  - Key: MONGO_HADOOP_CORE_FILE
    Name: arg
    Value: mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_CORE_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-core/${MONGO_HADOOP_VERSION}/${MONGO_HADOOP_CORE_FILE}
  - Key: MONGO_HADOOP_PIG_FILE
    Name: arg
    Value: mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_PIG_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-pig/${MONGO_HADOOP_VERSION}/${MONGO_HADOOP_PIG_FILE}
  - Key: MONGO_HADOOP_HIVE_FILE
    Name: arg
    Value: mongo-hadoop-hive-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_HIVE_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-hive/${MONGO_HADOOP_VERSION}/${MONGO_HADOOP_HIVE_FILE}
  - Key: MONGO_HADOOP_SPARK_FILE
    Name: arg
    Value: mongo-hadoop-spark-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_SPARK_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-spark/${MONGO_HADOOP_VERSION}/${MONGO_HADOOP_SPARK_FILE}
  - Key: MONGO_HADOOP_STREAMING_FILE
    Name: arg
    Value: mongo-hadoop-streaming-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_STREAMING_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-streaming/${MONGO_HADOOP_VERSION}/${MONGO_HADOOP_STREAMING_FILE}
  - Key: COCKROACH_BASE_URL
    Name: arg
    Value: https://binaries.cockroachdb.com
  - Key: COCKROACH_FILE
    Name: arg
    Value: cockroach-v${COCKROACH_VERSION}.linux-amd64.tgz
  - Key: COCKROACH_URL
    Name: arg
    Value: ${COCKROACH_BASE_URL}/${COCKROACH_FILE}
  - Key: COCKROACH_FOLDER
    Name: arg
    Value: cockroach-v${COCKROACH_VERSION}.linux-amd64
  - Key: POSTGRES_JDBC_BASE_URL
    Name: arg
    Value: https://jdbc.postgresql.org/download
  - Key: POSTGRES_JDBC_FILE
    Name: arg
    Value: postgresql-${POSTGRES_JDBC_VERSION}.jar
  - Key: POSTGRES_JDBC_URL
    Name: arg
    Value: ${POSTGRES_JDBC_BASE_URL}/${POSTGRES_JDBC_FILE}
  - Key: HIVE_HCATALOG_HBASE_STORAGE_HANDLER_FILE
    Name: arg
    Value: hive-hcatalog-hbase-storage-handler-${HIVE_HCATALOG_HBASE_STORAGE_HANDLER_VERSION}.jar
  - Key: HIVE_HCATALOG_HBASE_STORAGE_HANDLER_URL
    Name: arg
    Value: http://central.maven.org/maven2/org/apache/hive/hcatalog/hive-hcatalog-hbase-storage-handler/${HIVE_HCATALOG_HBASE_STORAGE_HANDLER_VERSION}/${HIVE_HCATALOG_HBASE_STORAGE_HANDLER_FILE}
  - Key: FINDSPARK_GIT
    Name: arg
    Value: https://github.com/minrk/findspark.git
  - Key: SPARK_HBASE_GIT
    Name: arg
    Value: https://github.com/hortonworks-spark/shc.git
  - Key: SPARK_XML_GIT
    Name: arg
    Value: https://github.com/databricks/spark-xml.git
  - Key: MONGO_REPO_URL
    Name: arg
    Value: http://repo.mongodb.org/apt/ubuntu
  - Key: CASSANDRA_URL
    Name: arg
    Value: http://www.apache.org/dist/cassandra
  - Chown: ""
    Name: add
    SourcesAndDest:
    - examples
    - /examples
  - Chown: ""
    Name: add
    SourcesAndDest:
    - datasets
    - /examples
  - Chown: ""
    Name: add
    SourcesAndDest:
    - conf
    - /conf
  - Chown: ""
    Name: add
    SourcesAndDest:
    - scripts
    - /scripts
  - Chown: ""
    Name: add
    SourcesAndDest:
    - built
    - /built
  - Chown: ""
    Name: add
    SourcesAndDest:
    - VERSION
    - /conf
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/jars
    - /jars/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/git
    - /git/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/pip
    - /pip/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${HADOOP_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${HIVE_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${PIG_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${SPARK_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${ZOOKEEPER_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${SQOOP_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${FLUME_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${FLINK_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${HBASE_FILE}*
    - /usr/local/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - downloads/foo
    - downloads/tars/${COCKROACH_FILE}*
    - /usr/local/
  - Key: DEBIAN_FRONTEND
    Name: arg
    Value: noninteractive
  - Name: user
    User: root
  - Env:
    - Key: BOOTSTRAP
      Value: /etc/bootstrap.sh
    Name: env
  - Env:
    - Key: JAVA_HOME
      Value: /usr/lib/jvm/java-8-oracle
    Name: env
  - Env:
    - Key: HADOOP_HOME
      Value: /usr/local/hadoop
    Name: env
  - Env:
    - Key: PIG_HOME
      Value: /usr/local/pig
    Name: env
  - Env:
    - Key: HIVE_HOME
      Value: /usr/local/hive
    Name: env
  - Env:
    - Key: HCAT_HOME
      Value: /usr/local/hive/hcatalog
    Name: env
  - Env:
    - Key: ZOOKEEPER_HOME
      Value: /usr/local/zookeeper/
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /usr/local/spark
    Name: env
  - Env:
    - Key: SPARK_CLASSPATH
      Value: '''/usr/local/spark/conf/mysql-connector-java.jar:$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:$HIVE_HOME/lib/slf4j-api-*.jar'''
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: ${SPARK_HOME}/python/:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):${PYTHONPATH}
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: ${SPARK_HOME}/python:${SPARK_HOME}/python/lib:${SPARK_HOME}/python/lib/py4j-0.10.7-src.zip
    Name: env
  - Env:
    - Key: HBASE_HOME
      Value: /usr/local/hbase
    Name: env
  - Env:
    - Key: HBASE_CONF_DIR
      Value: $HBASE_HOME/conf
    Name: env
  - Env:
    - Key: HDFS_NAMENODE_USER
      Value: root
    Name: env
  - Env:
    - Key: HDFS_DATANODE_USER
      Value: root
    Name: env
  - Env:
    - Key: HDFS_SECONDARYNAMENODE_USER
      Value: root
    Name: env
  - Env:
    - Key: YARN_RESOURCEMANAGER_USER
      Value: root
    Name: env
  - Env:
    - Key: YARN_NODEMANAGER_USER
      Value: root
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PIG_HOME/bin:$HIVE_HOME/bin:$ZOOKEEPER_HOME:bin:$SPARK_HOME/bin:$HBASE_HOME/bin
    Name: env
  - CmdLine:
    - "echo \"# ---------------------------------------------\" &&     echo \"# Make
      scripts executable\" &&     echo \"# ---------------------------------------------\"
      &&     chmod +x /scripts/*.sh &&     echo \"# ---------------------------------------------\"
      &&     echo \"# download jars\" &&     echo \"# this function will download
      the jar files if they are not there from the ADD command above\" &&     echo
      \"# ---------------------------------------------\" && \tcd /jars && \tdownload()
      { echo $1; test -e $1 && echo \"exists\" || curl --progress-bar $2 > $1; } &&
      \tdownload ${POSTGRES_JDBC_FILE} ${POSTGRES_JDBC_URL} && \tdownload ${MONGO_JAVA_DRIVER_FILE}
      ${MONGO_JAVA_DRIVER_URL} && \tdownload ${MONGO_HADOOP_CORE_FILE} ${MONGO_HADOOP_CORE_URL}
      && \tdownload ${MONGO_HADOOP_PIG_FILE} ${MONGO_HADOOP_PIG_URL} && \tdownload
      ${MONGO_HADOOP_HIVE_FILE} ${MONGO_HADOOP_HIVE_URL} && \tdownload ${MONGO_HADOOP_SPARK_FILE}
      ${MONGO_HADOOP_SPARK_URL} && \tdownload ${MONGO_HADOOP_STREAMING_FILE} ${MONGO_HADOOP_STREAMING_URL}
      && \tdownload ${HIVE_HCATALOG_HBASE_STORAGE_HANDLER_FILE} ${HIVE_HCATALOG_HBASE_STORAGE_HANDLER_URL}
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# download pip\" &&     echo \"# this function will download the pip files
      for python 2 & 3 if they are not there from the ADD command above\" &&     echo
      \"# ---------------------------------------------\" && \tcd /pip && \tdownload()
      { echo $1; test -e $1 && echo \"$1 exists\" || { mkdir -p $1; cd $1; mkdir -p
      2; cd 2; pip2 download $1; cd ..; mkdir -p 3; cd 3; pip3 download $1; cd ../..;
      } } && \tdownload pymongo && \tdownload pyhive && \tdownload happybase && \tdownload
      psycopg2 && \tdownload cassandra-driver && \tcd /home &&     echo \"# ---------------------------------------------\"
      &&     echo \"# passwordless ssh\" &&     echo \"# ---------------------------------------------\"
      &&     chmod 0777 /examples &&     rm -f /etc/ssh/ssh_host_dsa_key /etc/ssh/ssh_host_rsa_key
      /root/.ssh/id_rsa &&     ssh-keygen -q -N \"\" -t dsa -f /etc/ssh/ssh_host_dsa_key
      &&     ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key &&     ssh-keygen
      -q -N \"\" -t rsa -f /root/.ssh/id_rsa &&     cp /root/.ssh/id_rsa.pub /root/.ssh/authorized_keys
      &&     cd /scripts &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Make folders for HDFS data\" &&     echo \"# ---------------------------------------------\"
      &&     mkdir /data/hdfs &&     ls -l /usr/local &&     cd /usr/local &&     echo
      \"# ---------------------------------------------\" &&     echo \"# make install
      tar function\" &&     echo \"# this function will download the jar files if
      they are not there from the ADD command above and untar them to /usr/local\"
      &&     echo \"# it will also make the friendly symlink name to /usr/local/XXX
      and /XXX whether it was downloaded already or not\" &&     echo \"# install
      hadoop $HADOOP_FILE $HADOOP_URL $HADOOP_FOLDER\" &&     echo \"# ---------------------------------------------\"
      &&     install() { echo $1 $2 $3 $4; test -e /usr/local/${4} && echo \"already
      downloaded\"  || curl --progress-bar ${3} | tar -xz -C /usr/local ; test -e
      /usr/local/$1 && echo \"symlink exists\" || ln -s /usr/local/$4 /usr/local/$1;
      test -e /$1 && echo \"symlink exists\" || ln -s /usr/local/$4 /$1; }  &&     echo
      \"# ---------------------------------------------\" &&     echo \"# Hadoop\"
      &&     echo \"# ---------------------------------------------\" &&     install
      \"hadoop\" $HADOOP_FILE $HADOOP_URL $HADOOP_FOLDER &&     echo \"# ---------------------------------------------\"
      &&     echo \"# make backup of default config file and make symlink to my uploaded
      premade config files\" &&     echo \"# ---------------------------------------------\"
      && \tmv /usr/local/hadoop/etc/hadoop /usr/local/hadoop/etc/hadoop_backup &&
      \tmv /etc/my.cnf /etc/my.cnf.bak && \tln -s /conf/my.cnf /etc/my.cnf && \tln
      -s /conf/hadoop /usr/local/hadoop/etc/hadoop && \tln -s /conf/hadoop /usr/local/hadoop/conf
      && \tmv /conf/ssh_config /root/.ssh/config &&     chmod 600 /root/.ssh/config
      &&     chown root:root /root/.ssh/config &&     ln -s /conf/bootstrap-mysql.sh
      /etc/bootstrap.sh &&     chown root:root /etc/bootstrap.sh &&     chmod 700
      /etc/bootstrap.sh &&     chown root:root /conf/bootstrap-mysql.sh &&     chmod
      700 /conf/bootstrap-mysql.sh &&     chown root:root /conf/bootstrap-postgres.sh
      &&     chmod 700 /conf/bootstrap-postgres.sh &&     chmod 700 /scripts/start-hadoop.sh
      &&     chmod 700 /scripts/stop-hadoop.sh &&     chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh
      &&     chmod +x /scripts/loglevel-debug.sh &&     chmod +x /scripts/loglevel-info.sh
      &&     chmod +x /scripts/loglevel-warn.sh &&     chmod +x /scripts/loglevel-error.sh
      &&     echo \"\" &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Hive\" &&     echo \"# ---------------------------------------------\"
      &&     install \"hive\" $HIVE_FILE $HIVE_URL $HIVE_FOLDER &&     echo \"# ---------------------------------------------\"
      &&     echo \"# need the mysql connector in the hive folder in order to make
      the metastore work right\" &&     echo \"# ---------------------------------------------\"
      &&     ln -s /usr/share/java/mysql-connector-java.jar /usr/local/hive/lib/mysql-connector-java.jar
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# need the postgres connector in the hive folder in order to make the metastore
      work right\" &&     echo \"# ---------------------------------------------\"
      &&     ln -s /jars/postgresql* /usr/local/hive/jdbc &&    ln -s /jars/postgresql*
      /usr/local/hive/lib &&     echo \"# ---------------------------------------------\"
      &&     echo \"# make a friends symlink name for the hive-hcatalog-core.jar\"
      &&     echo \"# ---------------------------------------------\" &&     ln -s
      /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core-${HIVE_VERSION}.jar
      /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core.jar &&     echo \"#
      ---------------------------------------------\" &&     echo \"# backup default
      config and make symlink to my uploaded premade config files\" &&     echo \"#
      ---------------------------------------------\" &&     mv /usr/local/hive/conf
      /usr/local/hive/conf_backup &&     ln -s /conf/hive /usr/local/hive/conf &&
      \    echo \"# ---------------------------------------------\" &&     echo \"#
      Pig \" &&     echo \"# ---------------------------------------------\" &&     install
      \"pig\" $PIG_FILE $PIG_URL $PIG_FOLDER &&     echo \"# ---------------------------------------------\"
      &&     echo \"# backup default config and make symlink to my uploaded premade
      config files\" &&     echo \"# ---------------------------------------------\"
      &&     mv /usr/local/pig/conf /usr/local/pig/conf_backup &&     ln -s /conf/pig
      /usr/local/pig/conf &&     mkdir /usr/local/hive/hcatalog/lib &&     echo \"#
      ---------------------------------------------\" &&     echo \"# copy lib files
      needed for pig to use hcatalog\" &&     echo \"# ---------------------------------------------\"
      &&     ln -s /conf/hive-hcatalog-hbase-storage-handler-0.13.1.jar /usr/local/hive/hcatalog/lib
      &&     ln -s /conf/slf4j-api-1.6.0.jar /usr/local/hive/lib &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Spark\" &&     echo \"# ---------------------------------------------\"
      &&     install \"spark\" $SPARK_FILE $SPARK_URL $SPARK_FOLDER &&     echo \"#
      ---------------------------------------------\" &&     echo \"# backup default
      config and make symlink to my uploaded premade config files\" &&     echo \"#
      ---------------------------------------------\" &&     mv /usr/local/spark/conf
      /usr/local/spark/conf_backup &&     ln -s /conf/spark /usr/local/spark/conf
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# make symlink so spark can use hcatalog\" &&     echo \"# ---------------------------------------------\"
      &&     ln -s /usr/share/java/mysql-connector-java.jar /usr/local/spark/jars/mysql-connector-java.jar
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Sqoop \" &&     echo \"# ---------------------------------------------\"
      &&     install \"sqoop\" $SQOOP_FILE $SQOOP_URL $SQOOP_FOLDER &&     echo \"#
      ---------------------------------------------\" &&     echo \"# Flume \" &&
      \    echo \"# ---------------------------------------------\" &&     install
      \"flume\" $FLUME_FILE $FLUME_URL $FLUME_FOLDER &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Flink \" &&     echo \"# ---------------------------------------------\"
      &&     install \"flink\" $FLINK_FILE $FLINK_URL $FLINK_FOLDER &&     echo \"#
      ---------------------------------------------\" &&     echo \"# HBase\" &&     echo
      \"# ---------------------------------------------\" &&     install \"hbase\"
      $HBASE_FILE $HBASE_URL $HBASE_FOLDER &&     echo \"# ---------------------------------------------\"
      &&     echo \"# backup default config and make symlink to my uploaded premade
      config files\" &&     echo \"# ---------------------------------------------\"
      &&     mv /usr/local/hbase/conf /usr/local/hbase/conf_backup &&     ln -s /conf/hbase
      /usr/local/hbase/conf &&     ln -s /usr/local/hbase/bin/start-hbase.sh /scripts/starthbase.sh
      &&     ln -s /usr/local/hbase/bin/stop-hbase.sh /scripts/stophbase.sh &&     pip2
      install happybase psycopg2 &&     pip3 install happybase psycopg2 &&     echo
      \"# ---------------------------------------------\" &&     echo \"# Zookeeper\"
      &&     echo \"# ---------------------------------------------\" &&     install
      \"zookeeper\" $ZOOKEEPER_FILE $ZOOKEEPER_URL $ZOOKEEPER_FOLDER &&     mkdir
      /usr/local/zookeeper/data &&     echo \"# ---------------------------------------------\"
      &&     echo \"# backup default config and make symlink to my uploaded premade
      config files\" &&     echo \"# ---------------------------------------------\"
      &&     mv /usr/local/zookeeper/conf /usr/local/zookeeper/conf_backup &&     ln
      -s /conf/zookeeper /usr/local/zookeeper/conf &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Cockroach DB\" &&     echo \"# ---------------------------------------------\"
      &&     install \"cockroach\" $COCKROACH_FILE $COCKROACH_URL $COCKROACH_FOLDER
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Mongo & Cassandra Keys\" &&     echo \"# ---------------------------------------------\"
      &&     apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6
      &&     echo \"deb [ arch=amd64,arm64 ] ${MONGO_REPO_URL} xenial/mongodb-org/3.4
      multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list &&     echo
      \"deb ${CASSANDRA_URL}/debian ${CASSANDRA_VERSION}x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list
      &&     curl ${CASSANDRA_URL}/KEYS | sudo apt-key add - &&     apt-get update
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Mongo\" &&     echo \"# ---------------------------------------------\"
      &&     apt-get -y install mongodb-org &&     cd /pip &&     pip2 install pymongo
      &&     pip3 install pymongo &&     mkdir /data/mongo &&     mkdir /data/mongo/data
      &&     chmod +x /scripts/start-mongo.sh &&     chmod +x /scripts/stop-mongo.sh
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Cassandra\" &&     echo ${CASSANDRA_URL} &&     echo \"# ---------------------------------------------\"
      &&     apt-get -y install cassandra &&     chmod +x /scripts/start-cassandra.sh
      &&     chmod +x /scripts/stop-cassandra.sh &&     echo \"# change the data and
      log folder\" &&     mkdir /data/cassandra &&     mkdir /data/cassandra/data
      &&     mkdir /data/cassandra/log &&     mv /etc/cassandra /etc/cassandra_backup
      &&     ln -s /conf/cassandra /etc/cassandra &&     chmod +x /examples/cassandra/test-cassandra-table.py
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Postgresql\" &&     echo \"# ---------------------------------------------\"
      &&     apt-get -yq install postgresql postgresql-contrib postgresql-client &&
      \    chmod +x /scripts/start-postgres.sh &&     chmod +x /scripts/stop-postgres.sh
      &&     chmod +x /scripts/postgres-client.sh &&     /etc/init.d/postgresql start
      &&     sudo -u postgres psql -c \"create user root with password ''; alter user
      root with SUPERUSER;\" &&     sudo -u postgres psql -c \"create database root;\"
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Spark XML library\" &&     echo \"# ---------------------------------------------\"
      && \tmv /built/spark-xml* /usr/local/spark/jars &&     ln -s /usr/local/spark/jars/spark-xml*
      /usr/local/spark/jars/spark-xml.jar &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Spark HBase\" &&     echo ${SPARK_HBASE_GIT} &&     echo \"#
      ---------------------------------------------\" &&     mv /built/shc-core*  /usr/local/spark/jars
      &&     ln -s /usr/local/spark/jars/shc* /usr/local/spark/jars/shc.jar &&     echo
      \"# ---------------------------------------------\" &&     echo \"# Cassandra
      libraries\" &&     echo \"# ---------------------------------------------\"
      &&     pip2 install cassandra-driver &&     pip3 install cassandra-driver &&
      \    echo \"# ---------------------------------------------\" &&     echo \"#
      Helper scripts\" &&     echo \"# ---------------------------------------------\"
      &&     chmod +x /scripts/create-datadirs.sh &&     chmod +x /scripts/delete-datadirs.sh
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# FindSpark\" &&     echo \"# ---------------------------------------------\"
      &&     cd /tmp && \tgit clone https://github.com/minrk/findspark.git && \tcd
      /tmp/findspark &&     python2 setup.py install && \tpython3 setup.py install
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Miscellaneous\" &&     echo \"# ---------------------------------------------\"
      &&     echo \"alias hist='f(){ history | grep \\\"\\$1\\\";  unset -f f; };
      f'\" >> ~/.bashrc &&     echo \"alias pyspark0='python -i -c\\\"exec(\\\\\\\"from
      initSpark import initspark, hdfsPath\\nsc, spark, conf = initspark()\\nfrom
      pyspark.sql.types import *\\\\\\\")\\\"'\" >> ~/.bashrc &&     echo \"ARG PIG_OPTS=-Dhive.metastore.uris=thrift://bigdata:9083\"
      >> ~/.bashrc &&     echo \"ARG PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:$HIVE_HOME/lib/slf4j-api-*.jar\"
      >> ~/.bashrc &&     echo \"ARG HCAT_HOME=/usr/local/hive/hcatalog\" >> ~/.bashrc
      && \techo \"# Final Cleanup\" &&     apt-get -y clean &&     apt-get -y autoremove
      &&     rm -rf /var/lib/apt/lists/* && \tcd /home && \trm -r /tmp/findspark &&
      \    echo \"*************\" &&     echo \"\" >> /scripts/notes.txt"
    Name: run
    PrependShell: true
  - CmdLine:
    - /etc/bootstrap.sh
    - -d
    Name: cmd
    PrependShell: false
  From:
    Image: joegagliardo/ubuntu
  Name: ""
  Platform: ""
  SourceCode: FROM joegagliardo/ubuntu
