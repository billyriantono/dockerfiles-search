MetaArgs: null
Stages:
- BaseName: joegagliardo/ubuntu
  Commands:
  - Maintainer: joegagliardo
    Name: maintainer
  - Name: expose
    Ports:
    - "10000"
    - "10001"
    - "10002"
    - "22"
    - "3306"
    - "49707"
    - "50010"
    - "50020"
    - "50070"
    - "50075"
    - "50090"
    - "60000"
    - "60010"
    - "7077"
    - "7078"
    - "8030"
    - "8031"
    - "8032"
    - "8033"
    - "8040"
    - "8041"
    - "8042"
    - "8088"
    - "9042"
    - "9083"
    - "9160"
  - Key: HIVEUSER_PASSWORD
    Name: arg
    Value: hivepassword
  - Key: HIVE_METASTORE
    Name: arg
    Value: hivemetastore
  - Chown: ""
    Name: add
    SourcesAndDest:
    - examples
    - /examples
  - Chown: ""
    Name: add
    SourcesAndDest:
    - datasets
    - /examples
  - Chown: ""
    Name: add
    SourcesAndDest:
    - conf
    - /conf
  - Chown: ""
    Name: add
    SourcesAndDest:
    - scripts
    - /scripts
  - Key: HADOOP_VERSION
    Name: arg
    Value: 2.9.0
  - Key: PIG_VERSION
    Name: arg
    Value: 0.17.0
  - Key: HIVE_VERSION
    Name: arg
    Value: 2.3.3
  - Key: SPARK_VERSION
    Name: arg
    Value: 2.3.0
  - Key: ZOOKEEPER_VERSION
    Name: arg
    Value: 3.4.11
  - Key: HBASE_VERSION
    Name: arg
    Value: 1.4.3
  - Key: MONGO_VERSION
    Name: arg
    Value: 3.6.3
  - Key: MONGO_JAVA_DRIVER_VERSION
    Name: arg
    Value: 3.6.3
  - Key: MONGO_HADOOP_VERSION
    Name: arg
    Value: 2.0.2
  - Key: CASSANDRA_VERSION
    Name: arg
    Value: "311"
  - Key: SPARK_CASSANDRA_VERSION
    Name: arg
    Value: 2.0.7-s_2.11
  - Key: COCKROACH_VERSION
    Name: arg
    Value: 2.0.0
  - Key: HADOOP_BASE_URL
    Name: arg
    Value: http://mirrors.sonic.net/apache/hadoop/common
  - Key: HADOOP_URL
    Name: arg
    Value: ${HADOOP_BASE_URL}/hadoop-${HADOOP_VERSION}/hadoop-${HADOOP_VERSION}.tar.gz
  - Key: PIG_BASE_URL
    Name: arg
    Value: http://apache.claz.org/pig
  - Key: PIG_URL
    Name: arg
    Value: ${PIG_BASE_URL}/pig-${PIG_VERSION}/pig-${PIG_VERSION}.tar.gz
  - Key: HIVE_BASE_URL
    Name: arg
    Value: http://apache.claz.org/hive
  - Key: HIVE_URL
    Name: arg
    Value: ${HIVE_BASE_URL}/hive-${HIVE_VERSION}/apache-hive-${HIVE_VERSION}-bin.tar.gz
  - Key: SPARK_BASE_URL
    Name: arg
    Value: http://apache.claz.org/spark
  - Key: SPARK_URL
    Name: arg
    Value: ${SPARK_BASE_URL}/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
  - Key: ZOOKEEPER_BASE_URL
    Name: arg
    Value: http://apache.claz.org/zookeeper/
  - Key: ZOOKEEPER_URL
    Name: arg
    Value: ${ZOOKEEPER_BASE_URL}/zookeeper-${ZOOKEEPER_VERSION}/zookeeper-${ZOOKEEPER_VERSION}.tar.gz
  - Key: HBASE_BASE_URL
    Name: arg
    Value: http://apache.mirrors.pair.com/hbase
  - Key: HBASE_URL
    Name: arg
    Value: ${HBASE_BASE_URL}/${HBASE_VERSION}/hbase-${HBASE_VERSION}-bin.tar.gz
  - Key: MONGO_BASE_URL
    Name: arg
    Value: https://fastdl.mongodb.org/linux
  - Key: MONGO_URL
    Name: arg
    Value: ${MONGO_BASE_URL}/mongodb-linux-x86_64-${MONGO_VERSION}.tgz
  - Key: MONGO_JAVA_DRIVER_BASE_URL
    Name: arg
    Value: https://repo1.maven.org/maven2/org/mongodb
  - Key: MONGO_JAVA_DRIVER_URL
    Name: arg
    Value: ${MONGO_JAVA_DRIVER_BASE_URL}/mongo-java-driver/${MONGO_JAVA_DRIVER_VERSION}/mongo-java-driver-${MONGO_JAVA_DRIVER_VERSION}.jar
  - Key: MONGO_HADOOP_BASE_URL
    Name: arg
    Value: https://repo1.maven.org/maven2/org/mongodb/mongo-hadoop
  - Key: MONGO_HADOOP_CORE_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-core/${MONGO_HADOOP_VERSION}/mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_HIVE_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-hive/${MONGO_HADOOP_VERSION}/mongo-hadoop-hive-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_PIG_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-pig/${MONGO_HADOOP_VERSION}/mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_SPARK_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-spark/${MONGO_HADOOP_VERSION}/mongo-hadoop-spark-${MONGO_HADOOP_VERSION}.jar
  - Key: MONGO_HADOOP_STREAMING_URL
    Name: arg
    Value: ${MONGO_HADOOP_BASE_URL}/mongo-hadoop-streaming/${MONGO_HADOOP_VERSION}/mongo-hadoop-streaming-${MONGO_HADOOP_VERSION}.jar
  - Key: CASSANDRA_URL
    Name: arg
    Value: http://www.apache.org/dist/cassandra
  - Key: SPARK_CASSANDRA_BASE_URL
    Name: arg
    Value: http://dl.bintray.com/spark-packages/maven/datastax/spark-cassandra-connector
  - Key: SPARK_CASSANDRA_URL
    Name: arg
    Value: ${SPARK_CASSANDRA_BASE_URL}/${SPARK_CASSANDRA_VERSION}/spark-cassandra-connector-${SPARK_CASSANDRA_VERSION}.jar
  - Key: SPARK_CASSANDRA_FILE
    Name: arg
    Value: spark-cassandra-connector-${SPARK_CASSANDRA_VERSION}.jar
  - Key: SPARK_HBASE_GIT
    Name: arg
    Value: https://github.com/hortonworks-spark/shc.git
  - Key: SPARK_XML_GIT
    Name: arg
    Value: https://github.com/databricks/spark-xml.git
  - Key: MONGO_REPO_URL
    Name: arg
    Value: http://repo.mongodb.org/apt/ubuntu
  - Key: COCKROACH_BASE_URL
    Name: arg
    Value: https://binaries.cockroachdb.com
  - Key: COCKROACH_URL
    Name: arg
    Value: ${COCKROACH_BASE_URL}/cockroach-v${COCKROACH_VERSION}.linux-amd64.tgz
  - Name: user
    User: root
  - Env:
    - Key: BOOTSTRAP
      Value: /etc/bootstrap.sh
    Name: env
  - Env:
    - Key: JAVA_HOME
      Value: /usr/lib/jvm/java-8-oracle
    Name: env
  - Env:
    - Key: HADOOP_PREFIX
      Value: /usr/local/hadoop
    Name: env
  - Env:
    - Key: PIG_HOME
      Value: /usr/local/pig
    Name: env
  - Env:
    - Key: HIVE_HOME
      Value: /usr/local/hive
    Name: env
  - Env:
    - Key: HCAT_HOME
      Value: /usr/local/hive/hcatalog
    Name: env
  - Env:
    - Key: ZOOKEEPER_HOME
      Value: /usr/local/zookeeper/
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /usr/local/spark
    Name: env
  - Env:
    - Key: SPARK_CLASSPATH
      Value: '''/usr/local/spark/conf/mysql-connector-java.jar:$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:$HIVE_HOME/lib/slf4j-api-*.jar'''
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: ${SPARK_HOME}/python/:$(echo ${SPARK_HOME}/python/lib/py4j-*-src.zip):${PYTHONPATH}
    Name: env
  - Env:
    - Key: HBASE_HOME
      Value: /usr/local/hbase
    Name: env
  - Env:
    - Key: HBASE_CONF_DIR
      Value: $HBASE_HOME/conf
    Name: env
  - Env:
    - Key: PATH
      Value: $PATH:$HADOOP_PREFIX/bin:$HADOOP_PREFIX/sbin:$PIG_HOME/bin:$HIVE_HOME/bin:$ZOOKEEPER_HOME:bin:$SPARK_HOME/bin:$HBASE_HOME/bin
    Name: env
  - CmdLine:
    - "echo \"# ---------------------------------------------\" &&     echo \"# passwordless
      ssh\" &&     echo \"# ---------------------------------------------\" &&     chmod
      0777 /examples &&     apt-get update &&     rm -f /etc/ssh/ssh_host_dsa_key
      /etc/ssh/ssh_host_rsa_key /root/.ssh/id_rsa &&     ssh-keygen -q -N \"\" -t
      dsa -f /etc/ssh/ssh_host_dsa_key &&     ssh-keygen -q -N \"\" -t rsa -f /etc/ssh/ssh_host_rsa_key
      &&     ssh-keygen -q -N \"\" -t rsa -f /root/.ssh/id_rsa &&     cp /root/.ssh/id_rsa.pub
      /root/.ssh/authorized_keys &&     cd /scripts &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Cockroach DB\" &&     echo \"# ---------------------------------------------\"
      &&     mkdir /cr &&     cd /cr &&     wget ${COCKROACH_URL} &&     tar xfz cockroach-*
      &&     mv cockroach-v${COCKROACH_VERSION}.linux-amd64/cockroach /usr/local/bin
      &&     echo \"#! /bin/sh\" > /scripts/start-cockroach.sh &&     echo \"cd /data\"
      >> /scripts/start-cockroach.sh &&     echo \"cockroach start --insecure --host=localhost
      &\" >> /scripts/start-cockroach.sh &&     chmod +x /scripts/start-cockroach.sh
      &&     echo \"#! /bin/sh\" > /scripts/cockroach-shell.sh &&     echo \"cd /data\"
      >> /scripts/cockroach-shell.sh &&     echo \"cockroach sql --insecure\" >> /scripts/cockroach-shell.sh
      &&     chmod +x /scripts/cockroach-shell.sh &&     cd / &&     rm -r /cr &&
      \    echo \"# ---------------------------------------------\" &&     echo \"#
      Postgresql\" &&     echo \"# ---------------------------------------------\"
      &&     DEBIAN_FRONTEND=noninteractive apt-get -yq install postgresql postgresql-contrib
      postgresql-client &&     chmod +x /scripts/start-postgres.sh &&     chmod +x
      /scripts/stop-postgres.sh &&     chmod +x /scripts/postgres-client.sh &&     /etc/init.d/postgresql
      start &&     sudo -u postgres psql -c \"create user root with password ''; alter
      user root with SUPERUSER;\" &&     sudo -u postgres psql -c \"create database
      root;\" &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Make folders for HDFS data\" &&     echo \"# ---------------------------------------------\"
      &&     mkdir /data/hdfs &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Hadoop\" &&     echo \"# ---------------------------------------------\"
      &&     echo ${HADOOP_URL} &&     curl -s ${HADOOP_URL} | tar -xz -C /usr/local/
      &&     cd /usr/local &&     ln -s /usr/local/hadoop-${HADOOP_VERSION} /usr/local/hadoop
      &&     ln -s /usr/local/hadoop-${HADOOP_VERSION} /hadoop &&     sed -i '/^export
      JAVA_HOME/ s:.*:export JAVA_HOME=/usr\\nexport HADOOP_PREFIX=/usr/local/hadoop\\nexport
      HADOOP_HOME=/usr/local/hadoop\\n:' $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh &&
      \    sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=/usr/local/hadoop/etc/hadoop/:'
      $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh && \tmv /usr/local/hadoop/etc/hadoop
      /usr/local/hadoop/etc/hadoop_backup && \tmv /etc/my.cnf /etc/my.cnf.bak && \tln
      -s /conf/my.cnf /etc/my.cnf && \tln -s /conf/hadoop /usr/local/hadoop/etc/hadoop
      && \tmv /conf/ssh_config /root/.ssh/config &&     chmod 600 /root/.ssh/config
      &&     chown root:root /root/.ssh/config &&     ln -s /conf/bootstrap-mysql.sh
      /etc/bootstrap.sh &&     chown root:root /etc/bootstrap.sh &&     chmod 700
      /etc/bootstrap.sh &&     chown root:root /conf/bootstrap-mysql.sh &&     chmod
      700 /conf/bootstrap-mysql.sh &&     chown root:root /conf/bootstrap-postgres.sh
      &&     chmod 700 /conf/bootstrap-postgres.sh &&     chmod 700 /scripts/start-hadoop.sh
      &&     chmod 700 /scripts/stop-hadoop.sh &&     ls -la /usr/local/hadoop/etc/hadoop/*-env.sh
      &&     chmod +x /usr/local/hadoop/etc/hadoop/*-env.sh &&     ls -la /usr/local/hadoop/etc/hadoop/*-env.sh
      &&     echo \"# fix the 254 error code\" &&     sed  -i \"/^[^#]*UsePAM/ s/.*/#&/\"
      \ /etc/ssh/sshd_config &&     echo \"UsePAM no\" >> /etc/ssh/sshd_config &&
      \    echo \"Port 2122\" >> /etc/ssh/sshd_config &&     service ssh start $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh
      $HADOOP_PREFIX/sbin/start-dfs.sh $HADOOP_PREFIX/bin/hdfs dfs -mkdir -p /user/root
      &&     service ssh start $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh $HADOOP_PREFIX/sbin/start-dfs.sh
      $HADOOP_PREFIX/bin/hdfs dfs -put $HADOOP_PREFIX/etc/hadoop/ input &&     chmod
      +x /scripts/loglevel-debug.sh &&     chmod +x /scripts/loglevel-info.sh &&     chmod
      +x /scripts/loglevel-warn.sh &&     chmod +x /scripts/loglevel-error.sh &&     echo
      \"# ---------------------------------------------\" &&     echo \"# Hive\" &&
      \    echo ${HIVE_URL} &&     echo \"# ---------------------------------------------\"
      &&     curl ${HIVE_URL} | tar -zx -C /usr/local &&     ln -s /usr/local/apache-hive-${HIVE_VERSION}-bin
      /usr/local/hive &&     ln -s /usr/share/java/mysql-connector-java.jar /usr/local/hive/lib/mysql-connector-java.jar
      &&     mv /usr/local/hive/conf /usr/local/hive/conf_backup &&     ln -s /conf/hive
      /usr/local/hive/conf &&     wget https://jdbc.postgresql.org/download/postgresql-42.1.3.jar
      &&     mv postgresql-42.1.3.jar /usr/local/hive/jdbc &&     cp /usr/local/hive/jdbc/postgresql-42.1.3.jar
      /usr/local/hive/lib &&     ln -s /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core-${HIVE_VERSION}.jar
      /usr/local/hive/hcatalog/share/hcatalog/hive-hcatalog-core.jar &&     echo \"#
      ---------------------------------------------\" &&     echo \"# Hiveserver2
      Python Package\" &&     echo \"# ---------------------------------------------\"
      &&     apt-get -y install libsasl2-dev &&     pip2 install PyHive &&     pip3
      install PyHive &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Pig \" &&     echo ${PIG_URL} &&     echo \"# ---------------------------------------------\"
      &&     curl ${PIG_URL} | tar -zx -C /usr/local &&     ln -s /usr/local/pig-${PIG_VERSION}
      /usr/local/pig &&     mv /usr/local/pig/conf /usr/local/pig/conf_backup &&     ln
      -s /conf/pig /usr/local/pig/conf &&     mkdir /usr/local/hive/hcatalog/lib &&
      \    ln -s /conf/hive-hcatalog-hbase-storage-handler-0.13.1.jar /usr/local/hive/hcatalog/lib
      &&     ln -s /conf/slf4j-api-1.6.0.jar /usr/local/hive/lib &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Make scripts executable\" &&     echo \"# ---------------------------------------------\"
      &&     chmod +x /scripts/format-namenode.sh &&     chmod +x /scripts/exit-safemode.sh
      &&     chmod +x /scripts/start-thrift.sh &&     chmod +x /scripts/init-schema-mysql.sh
      &&     chmod +x /scripts/init-schema-postgres.sh &&     chmod +x /scripts/start-everything.sh
      &&     chmod +x /scripts/stop-everything.sh &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Spark\" &&     echo ${SPARK_URL} &&     echo \"# ---------------------------------------------\"
      &&     curl ${SPARK_URL} | tar -zx -C /usr/local &&     ln -s /usr/local/spark-${SPARK_VERSION}-bin-hadoop2.7
      /usr/local/spark &&     ln -s /usr/local/hive/conf/hive-site.xml /usr/local/spark/conf/hive-site.xml
      &&     ln -s /usr/share/java/mysql-connector-java.jar /usr/local/spark/conf/mysql-connector-java.jar
      &&     ln -s /usr/share/java/mysql-connector-java.jar /usr/local/spark/jars/mysql-connector-java.jar
      &&     mv /usr/local/spark/conf /usr/local/spark/conf_backup &&     ln -s /conf/spark
      /usr/local/spark/conf &&     cd /home &&     echo \"# ---------------------------------------------\"
      &&     echo \"# HBase\" &&     echo ${HBASE_URL} &&     echo \"# ---------------------------------------------\"
      &&     curl ${HBASE_URL} | tar -zx -C /usr/local &&     ln -s /usr/local/hbase-${HBASE_VERSION}
      /usr/local/hbase &&     mv /usr/local/hbase/conf /usr/local/hbase/conf_backup
      &&    ln -s /conf/hbase /usr/local/hbase/conf &&     ln -s /usr/local/hbase/bin/start-hbase.sh
      /scripts/starthbase.sh &&    ln -s /usr/local/hbase/bin/stop-hbase.sh /scripts/stophbase.sh
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Zookeeper\" &&     echo ${ZOOKEEPER_URL} &&     echo \"# ---------------------------------------------\"
      &&     curl ${ZOOKEEPER_URL} | tar -zx -C /usr/local &&     ln -s /usr/local/zookeeper-${ZOOKEEPER_VERSION}
      /usr/local/zookeeper &&     mkdir /usr/local/zookeeper/data &&     mv /usr/local/zookeeper/conf
      /usr/local/zookeeper/conf_backup &&     ln -s /conf/zookeeper /usr/local/zookeeper/conf
      &&     pip2 install happybase psycopg2 &&     pip3 install happybase psycopg2
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Mongo & Cassandra Keys\" &&     echo \"# ---------------------------------------------\"
      &&     apt-key adv --keyserver hkp://keyserver.ubuntu.com:80 --recv 0C49F3730359A14518585931BC711F9BA15703C6
      &&     echo \"deb [ arch=amd64,arm64 ] ${MONGO_REPO_URL} xenial/mongodb-org/3.4
      multiverse\" | sudo tee /etc/apt/sources.list.d/mongodb-org-3.4.list &&     echo
      \"deb ${CASSANDRA_URL}/debian ${CASSANDRA_VERSION}x main\" | sudo tee -a /etc/apt/sources.list.d/cassandra.sources.list
      &&     curl ${CASSANDRA_URL}/KEYS | sudo apt-key add - &&     apt-get update
      &&     echo \"# ---------------------------------------------\" &&     echo
      \"# Mongo\" &&     echo \"# ---------------------------------------------\"
      &&     apt-get -y install mongodb-org &&     pip2 install pymongo &&     pip3
      install pymongo &&     mkdir /data/mongo &&     mkdir /data/mongo/data &&     chmod
      +x /scripts/start-mongo.sh &&     chmod +x /scripts/stop-mongo.sh &&     echo
      \"# ---------------------------------------------\" &&     echo \"# Cassandra\"
      &&     echo ${CASSANDRA_URL} &&     echo \"# ---------------------------------------------\"
      &&     apt-get -y install cassandra &&     chmod +x /scripts/start-cassandra.sh
      &&     chmod +x /scripts/stop-cassandra.sh &&     echo \"# change the data and
      log folder\" &&     mkdir /data/cassandra &&     mkdir /data/cassandra/data
      &&     mkdir /data/cassandra/log &&     mkdir /data/cassandra/saved_caches &&
      \    mv /etc/cassandra /etc/cassandra_backup &&     ln -s /conf/cassandra /etc/cassandra
      &&     chmod +x /examples/cassandra/test-cassandra-table.py &&     echo \"#
      ---------------------------------------------\" &&     echo \"# Cassandra libraries\"
      &&     echo \"# ---------------------------------------------\" &&     pip2
      install cassandra-driver &&     pip3 install cassandra-driver &&     echo \"#
      ---------------------------------------------\" &&     echo \"# Helper scripts\"
      &&     echo \"# ---------------------------------------------\" &&     chmod
      +x /scripts/create-datadirs.sh &&     chmod +x /scripts/delete-datadirs.sh &&
      \    echo \"# ---------------------------------------------\" &&     cd /home
      &&     echo \"# Spark Cassandra Connector\" &&     echo ${SPARK_CASSANDRA_URL}
      &&     echo \"# ---------------------------------------------\" && \twget ${SPARK_CASSANDRA_URL}
      &&     mv /home/${SPARK_CASSANDRA_FILE} /usr/local/spark/jars && \tln -s /usr/local/spark/jars/${SPARK_CASSANDRA_FILE}
      /usr/local/spark/jars/spark-cassandra-connector.jar &&     echo \"# ---------------------------------------------\"
      && \techo \"MONGO-HADOOP\" &&     echo \"# ---------------------------------------------\"
      && \tcd /home && \twget --content-disposition ${MONGO_HADOOP_CORE_URL} && \twget
      --content-disposition ${MONGO_HADOOP_PIG_URL} && \twget --content-disposition
      ${MONGO_HADOOP_HIVE_URL} && \twget --content-disposition ${MONGO_HADOOP_SPARK_URL}
      && \twget --content-disposition ${MONGO_HADOOP_STREAMING_URL} && \twget --content-disposition
      ${MONGO_JAVA_DRIVER_URL} && \tmkdir /usr/local/mongo-hadoop && \tmv mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop && \tmv mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop && \tmv mongo-hadoop-hive-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop && \tmv mongo-hadoop-spark-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop && \tmv mongo-hadoop-streaming-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop && \tmv mongo-java-driver-${MONGO_JAVA_DRIVER_VERSION}.jar
      /usr/local/mongo-hadoop && \tln -s /usr/local/mongo-hadoop/mongo-hadoop-core-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-hadoop-core.jar && \tln -s /usr/local/mongo-hadoop/mongo-hadoop-pig-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-hadoop-pig.jar && \tln -s /usr/local/mongo-hadoop/mongo-hadoop-hive-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-hadoop-hive.jar && \tln -s /usr/local/mongo-hadoop/mongo-hadoop-spark-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-hadoop-spark.jar && \tln -s /usr/local/mongo-hadoop/mongo-hadoop-streaming-${MONGO_HADOOP_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-hadoop-streaming.jar && \tln -s /usr/local/mongo-hadoop/mongo-java-driver-${MONGO_JAVA_DRIVER_VERSION}.jar
      /usr/local/mongo-hadoop/mongo-java-driver.jar && \tcd /usr/local/mongo-hadoop
      && \tgit clone https://github.com/mongodb/mongo-hadoop.git && \tcd /usr/local/mongo-hadoop/mongo-hadoop/spark/src/main/python
      && \tpython setup.py install && \tpython3 setup.py install &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Spark HBase\" &&     echo ${SPARK_HBASE_GIT} &&     echo \"#
      ---------------------------------------------\" &&     cd /tmp &&     git clone
      ${SPARK_HBASE_GIT} &&     cd /tmp/shc &&     mvn package -DskipTests &&     ln
      -s /usr/local/spark/jars/shc /usr/local/spark/jars/shc.jar &&     cd /tmp &&
      \    rm -r /tmp/shc &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Spark XML library\" &&     echo \"# ---------------------------------------------\"
      &&     cd /tmp &&     git clone ${SPARK_XML_GIT} &&     cd /tmp/spark-xml &&
      \    sbt/sbt package &&     cp /tmp/spark-xml/target/scala-2.11/*.jar /usr/local/spark/jars
      &&     ln -s /usr/local/spark/jars/spark-xml_2.11-0.4.1.jar /usr/local/spark/jars/spark-xml.jar
      &&     cd /tmp &&     rm -r /tmp/spark-xml &&     echo \"# ---------------------------------------------\"
      &&     echo \"# FindSpark\" &&     echo \"# ---------------------------------------------\"
      && \tgit clone https://github.com/minrk/findspark.git && \tcd /tmp/findspark
      &&     python2 setup.py install && \tpython3 setup.py install && \tcd /home
      && \trm -r /tmp/findspark &&     echo \"# ---------------------------------------------\"
      &&     echo \"# Miscellaneous\" &&     echo \"# ---------------------------------------------\"
      &&     echo \"alias hist='f(){ history | grep \\\"\\$1\\\";  unset -f f; };
      f'\" >> ~/.bashrc &&     echo \"alias pyspark0='python -i -c\\\"exec(\\\\\\\"from
      initSpark import initspark, hdfsPath\\nsc, spark, conf = initspark()\\nfrom
      pyspark.sql.types import *\\\\\\\")\\\"'\" >> ~/.bashrc &&     echo \"export
      PIG_OPTS=-Dhive.metastore.uris=thrift://bigdata:9083\" >> ~/.bashrc &&     echo
      \"export PIG_CLASSPATH=$HCAT_HOME/share/hcatalog/hcatalog-core*.jar:$HCAT_HOME/share/hcatalog/hcatalog-pig-adapter*.jar:$HIVE_HOME/lib/hive-metastore-*.jar:$HIVE_HOME/lib/libthrift-*.jar:$HIVE_HOME/lib/hive-exec-*.jar:$HIVE_HOME/lib/libfb303-*.jar:$HIVE_HOME/lib/jdo2-api-*-ec.jar:$HIVE_HOME/conf:$HADOOP_HOME/conf:$HIVE_HOME/lib/slf4j-api-*.jar\"
      >> ~/.bashrc &&     echo \"export HCAT_HOME=/usr/local/hive/hcatalog\" >> ~/.bashrc
      && \techo \"# Final Cleanup\" &&     apt-get -y clean &&     apt-get -y autoremove
      &&     rm -rf /var/lib/apt/lists/* &&     echo \"*************\""
    Name: run
    PrependShell: true
  - CmdLine:
    - echo "*************" &&     echo "" >> /scripts/notes.txt
    Name: run
    PrependShell: true
  - CmdLine:
    - /etc/bootstrap.sh
    - -d
    Name: cmd
    PrependShell: false
  From:
    Image: joegagliardo/ubuntu
  Name: ""
  Platform: ""
  SourceCode: FROM joegagliardo/ubuntu
