MetaArgs: null
Stages:
- BaseName: ubuntu:xenial
  Commands:
  - Maintainer: Ruslan Korniichuk <ruslan.korniichuk@gmail.com>
    Name: maintainer
  - Name: user
    User: root
  - Env:
    - Key: OS_REFRESHED_AT
      Value: "2017-04-21"
    Name: env
  - CmdLine:
    - 'apt-get -qq update # -qq -- no output except for errors'
    Name: run
    PrependShell: true
  - Env:
    - Key: APT_REFRESHED_AT
      Value: "2017-04-21"
    Name: env
  - CmdLine:
    - apt-get -qq update         && apt-get install -y g++ nano pigz wget         &&
      apt-get clean
    Name: run
    PrependShell: true
  - Env:
    - Key: JAVA_REFRESHED_AT
      Value: "2017-04-21"
    Name: env
  - CmdLine:
    - apt-get -qq update && apt-get install -y openjdk-8* && apt-get clean
    Name: run
    PrependShell: true
  - Env:
    - Key: PYTHON_REFRESHED_AT
      Value: "2017-04-21"
    Name: env
  - CmdLine:
    - apt-get -qq update         && apt-get install -y python python-dev         &&
      apt-get clean
    Name: run
    PrependShell: true
  - Env:
    - Key: PIP_REFRESHED_AT
      Value: "2017-04-21"
    Name: env
  - CmdLine:
    - wget --directory-prefix /tmp https://bootstrap.pypa.io/get-pip.py
    Name: run
    PrependShell: true
  - CmdLine:
    - python /tmp/get-pip.py
    Name: run
    PrependShell: true
  - CmdLine:
    - rm /tmp/get-pip.py
    Name: run
    PrependShell: true
  - Env:
    - Key: URL_SCHEME
      Value: http
    Name: env
  - Env:
    - Key: URL_NETLOC
      Value: d3kbcqa49mib13.cloudfront.net
    Name: env
  - Env:
    - Key: URL_PATH
      Value: /spark-2.1.0-bin-hadoop2.7.tgz
    Name: env
  - Env:
    - Key: URL
      Value: $URL_SCHEME://$URL_NETLOC$URL_PATH
    Name: env
  - CmdLine:
    - wget --directory-prefix /tmp $URL
    Name: run
    PrependShell: true
  - CmdLine:
    - unpigz --to-stdout /tmp/spark-2.1.0-bin-hadoop2.7.tgz         | tar --extract
      --file - --directory /usr/local/src
    Name: run
    PrependShell: true
  - CmdLine:
    - rm /tmp/spark-2.1.0-bin-hadoop2.7.tgz
    Name: run
    PrependShell: true
  - Env:
    - Key: SPARK_HOME
      Value: /usr/local/src/spark-2.1.0-bin-hadoop2.7
    Name: env
  - Env:
    - Key: PYTHON_DIR_PATH
      Value: $SPARK_HOME/python/
    Name: env
  - Env:
    - Key: PY4J_PATH
      Value: $SPARK_HOME/python/lib/py4j-0.10.4-src.zip
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: $PYTHON_DIR_PATH:$PY4J_PATH
    Name: env
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - docker/log4j.properties
    - $SPARK_HOME/conf/log4j.properties
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - docker/spark-defaults.conf
    - $SPARK_HOME/conf/spark-defaults.conf
  - CmdLine:
    - useradd -c "Apache PySpark" -m -s /bin/bash pyspark
    Name: run
    PrependShell: true
  - CmdLine:
    - echo "pyspark:pyspark" | chpasswd
    Name: run
    PrependShell: true
  - Name: user
    User: pyspark
  - Name: workdir
    Path: /home/pyspark
  From:
    Image: ubuntu:xenial
  Name: ""
  Platform: ""
  SourceCode: FROM ubuntu:xenial
