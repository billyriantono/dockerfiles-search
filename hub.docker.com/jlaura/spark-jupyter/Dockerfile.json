{
  "MetaArgs": null,
  "Stages": [
    {
      "Name": "",
      "BaseName": "jupyter/scipy-notebook",
      "SourceCode": "FROM jupyter/scipy-notebook",
      "Platform": "",
      "From": {
        "Image": "jupyter/scipy-notebook"
      },
      "Commands": [
        {
          "Name": "user",
          "User": "root"
        },
        {
          "Env": [
            {
              "Key": "APACHE_SPARK_VERSION",
              "Value": "2.2.0"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "HADOOP_VERSION",
              "Value": "2.7"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "apt-get -y update \u0026\u0026     apt-get install --no-install-recommends -y openjdk-8-jre-headless ca-certificates-java \u0026\u0026     apt-get clean \u0026\u0026     rm -rf /var/lib/apt/lists/*"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "cd /tmp \u0026\u0026         wget -q http://d3kbcqa49mib13.cloudfront.net/spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz \u0026\u0026         echo \"7a186a2a007b2dfd880571f7214a7d329c972510a460a8bdbef9f7f2a891019343c020f74b496a61e5aa42bc9e9a79cc99defe5cb3bf8b6f49c07e01b259bc6b *spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz\" | sha512sum -c - \u0026\u0026         tar xzf spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz -C /usr/local \u0026\u0026         rm spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION}.tgz"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "cd /usr/local \u0026\u0026 ln -s spark-${APACHE_SPARK_VERSION}-bin-hadoop${HADOOP_VERSION} spark"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "SPARK_HOME",
              "Value": "/usr/local/spark"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "PYTHONPATH",
              "Value": "$SPARK_HOME/python:$SPARK_HOME/python/lib/py4j-0.10.4-src.zip"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_OPTS",
              "Value": "--driver-java-options=-Xms1024M --driver-java-options=-Xmx4096M --driver-java-options=-Dlog4j.logLevel=info"
            }
          ],
          "Name": "env"
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "environment.yml",
            "."
          ]
        },
        {
          "CmdLine": [
            "conda config --add channels usgs-astrogeology \u0026\u0026     conda config --add channels conda-forge \u0026\u0026     conda env update -n root -f environment.yml"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "pip install findspark"
          ],
          "Name": "run",
          "PrependShell": true
        }
      ]
    }
  ]
}