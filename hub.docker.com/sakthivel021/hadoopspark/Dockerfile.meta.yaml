MetaArgs: null
Stages:
- BaseName: centos:6
  Commands:
  - Name: volume
    Volumes:
    - opt-vol:/opt
  - Env:
    - Key: SPARK_VERSION
      Value: 2.4.0
    Name: env
  - Env:
    - Key: HADOOP_VERSION
      Value: 2.7.4
    Name: env
  - Env:
    - Key: HADOOP_PACKAGE
      Value: hadoop-$HADOOP_VERSION
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /opt/spark
    Name: env
  - Env:
    - Key: NOTVISIBLE
      Value: '"in users profile"'
    Name: env
  - Env:
    - Key: HADOOP_HOME
      Value: /opt/hadoop/
    Name: env
  - Env:
    - Key: HADOOP_PREFIX
      Value: /opt/hadoop
    Name: env
  - Env:
    - Key: SPARK_PACKAGE
      Value: spark-${SPARK_VERSION}-bin-hadoop2.7
    Name: env
  - CmdLine:
    - yum install -y openssh-server wget java openssh-clients vim     && cat /etc/redhat-release     &&
      service sshd start     && chkconfig sshd on     && echo 'root:$1$NFUWV7nM$L2G0.R82dulmo1m7Szobn/'
      | chpasswd -e     && sed -i 's/PermitRootLogin prohibit-password/PermitRootLogin
      yes/' /etc/ssh/sshd_config     && sed 's@session\s*required\s*pam_loginuid.so@session
      optional pam_loginuid.so@g' -i /etc/pam.d/sshd     && echo "export VISIBLE=now"
      >> /etc/profile     && yum install -y java-1.8.0-openjdk-devel     && java -version     &&
      rpm -qa | grep jdk      && mkdir -p /opt/yarn     && wget -O hadoop.tar.gz https://archive.apache.org/dist/hadoop/common/hadoop-2.7.4/hadoop-2.7.4.tar.gz     &&
      tar -xzf hadoop.tar.gz -C /opt/yarn  && rm hadoop.tar.gz     && ln -s /opt/yarn/$HADOOP_PACKAGE
      /opt/hadoop     && groupadd hadoop     && useradd -g hadoop yarn     && useradd
      -g hadoop hdfs     && useradd -g hadoop mapred     && mkdir -p /var/data/hadoop/hdfs/nn     &&
      mkdir -p /var/data/hadoop/hdfs/snn     && mkdir /var/data/hadoop/hdfs/dn     &&
      mkdir $HADOOP_HOME/logs     && chown -R hdfs:hadoop /opt/hadoop     && chown
      -R hdfs:hadoop /opt/yarn     && chown -R hdfs:hadoop /var/data/     && chmod
      777 /tmp     && mkdir -p /var/log/hadoop/logs     && chmod 755 /var/log/hadoop/logs     &&
      chown yarn:hadoop /var/log/hadoop/logs -R     && echo 'hdfs:$1$HMPEwAI3$.ToAzPVH2ijXi8weK8aJM0'
      | chpasswd -e     && echo 'yarn:$1$P9N9Q59u$ikIL28z4.y0fmP8D5qiCM1' | chpasswd
      -e     && echo 'mapred:$1$lEviz/bJ$fASeVRD7tcsFeVLyF0HN21' | chpasswd -e
    Name: run
    PrependShell: true
  - Name: user
    User: hdfs
  - CmdLine:
    - ssh-keygen -q -N "" -t rsa -f /home/hdfs/.ssh/id_rsa     && cp /home/hdfs/.ssh/id_rsa.pub
      /home/hdfs/.ssh/authorized_keys
    Name: run
    PrependShell: true
  - Name: user
    User: root
  - CmdLine:
    - "yum install -y wget     && wget -O spark.tar.gz http://apache.volia.net/spark/spark-2.4.0/spark-2.4.0-bin-hadoop2.7.tgz
      \    && mkdir -p /opt/     && tar xvf spark.tar.gz -C /opt/         && ln -s
      /opt/$SPARK_PACKAGE /opt/spark \t&& chown -R hdfs:hadoop /opt/$SPARK_PACKAGE
      \t&& chown -R hdfs:hadoop /opt/spark             &&mkdir /var/log/spark             &&
      chown -R hdfs:hadoop /var/log/spark                 && mkdir /tmp/spark                         &&
      echo \"spark.master    yarn\" >> /opt/spark/conf/spark-defaults.conf                         &&
      echo \"spark.driver.memory    512m\" >> /opt/spark/conf/spark-defaults.conf
      \                        && echo \"spark.executor.memory          512m\" >>
      /opt/spark/conf/spark-defaults.conf                         && echo \"spark.eventLog.enabled
      \ true\" >> /opt/spark/conf/spark-defaults.conf                         && echo
      \"spark.eventLog.dir /var/log/spark/\" >> /opt/spark/conf/spark-defaults.conf
      \                        && echo \"spark.history.provider            org.apache.spark.deploy.history.FsHistoryProvider\"
      >> /opt/spark/conf/spark-defaults.conf                         && echo \"spark.history.fs.update.interval
      \ 10s\" >> /opt/spark/conf/spark-defaults.conf                         && echo
      \"spark.history.ui.port             18080\" >> /opt/spark/conf/spark-defaults.conf"
    Name: run
    PrependShell: true
  - Name: expose
    Ports:
    - "4040"
  - Name: expose
    Ports:
    - "7077"
  - Name: expose
    Ports:
    - "8088"
  - Name: expose
    Ports:
    - "18080"
  - CmdLine:
    - ln -s $HADOOP_HOME/etc/hadoop $HADOOP_HOME/conf     && echo "export SPARK_HOME=/opt/spark"
      >> /etc/profile     && echo "export  HADOOP_HOME=/opt/hadoop" >> /etc/profile     &&
      echo "export PATH=$PATH:/opt/spark/bin:$HADOOP_HOME/bin:$HADOOP_HOME/sbin" >>
      /etc/profile     && echo "export  JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk.x86_64
      " >> /etc/profile     && echo "export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop"
      >> /etc/profile     && echo "export SPARK_HOME=/opt/spark" >> /etc/profile     &&
      echo "export LD_LIBRARY_PATH=/opt/hadoop/lib/native:$LD_LIBRARY_PATH" >> /etc/profile     &&
      echo "export HADOOP_HOME=/opt/hadoop/" >> /opt/hadoop/etc/hadoop/hadoop-env.sh     &&
      echo "export JAVA_HOME=/usr/lib/jvm/java-1.8.0-openjdk.x86_64" >> /opt/hadoop/etc/hadoop/hadoop-env.sh     &&
      echo "export HADOOP_CONF_DIR=/opt/hadoop/etc/hadoop" >> /opt/hadoop/etc/hadoop/hadoop-env.sh     &&
      echo "export HADOOP_OPTS=-Djava.net.preferIPv4Stack=true" >> /opt/hadoop/etc/hadoop/hadoop-env.sh     &&
      echo "hadoop-master" >> /opt/hadoop/etc/hadoop/masters
    Name: run
    PrependShell: true
  - Env:
    - Key: PATH
      Value: $PATH:$HADOOP_HOME/bin:$HADOOP_HOME/sbin
    Name: env
  - Env:
    - Key: HADOOP_PREFIX
      Value: $HADOOP_HOME
    Name: env
  - Env:
    - Key: HADOOP_COMMON_HOME
      Value: $HADOOP_HOME
    Name: env
  - Env:
    - Key: HADOOP_HDFS_HOME
      Value: $HADOOP_HOME
    Name: env
  - Env:
    - Key: HADOOP_MAPRED_HOME
      Value: $HADOOP_HOME
    Name: env
  - Env:
    - Key: HADOOP_YARN_HOME
      Value: $HADOOP_HOME
    Name: env
  - Env:
    - Key: HADOOP_CONF_DIR
      Value: $HADOOP_HOME/etc/hadoop
    Name: env
  - Env:
    - Key: YARN_CONF_DIR
      Value: $HADOOP_PREFIX/etc/hadoop
    Name: env
  - Name: user
    User: hdfs
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - .
    - /home/hdfs/
  - Chown: ""
    Name: add
    SourcesAndDest:
    - core-site.xml
    - $HADOOP_HOME/etc/hadoop/core-site.xml
  - Chown: ""
    Name: add
    SourcesAndDest:
    - hdfs-site.xml
    - $HADOOP_HOME/etc/hadoop/hdfs-site.xml
  - Chown: ""
    Name: add
    SourcesAndDest:
    - mapred-site.xml
    - $HADOOP_HOME/etc/hadoop/mapred-site.xml
  - Chown: ""
    Name: add
    SourcesAndDest:
    - yarn-site.xml
    - $HADOOP_HOME/etc/hadoop/yarn-site.xml
  - Chown: ""
    Name: add
    SourcesAndDest:
    - slaves
    - $HADOOP_HOME/etc/hadoop/slaves
  - CmdLine:
    - sed -i '/^export HADOOP_CONF_DIR/ s:.*:export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop/:'
      /opt/hadoop/etc/hadoop/hadoop-env.sh
    Name: run
    PrependShell: true
  - Name: expose
    Ports:
    - "10022:22"
  - Name: user
    User: root
  - CmdLine:
    - /usr/sbin/sshd
    - -D
    Name: cmd
    PrependShell: false
  From:
    Image: centos:6
  Name: ""
  Platform: ""
  SourceCode: FROM centos:6
