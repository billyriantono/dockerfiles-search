{
  "MetaArgs": null,
  "Stages": [
    {
      "Name": "",
      "BaseName": "rpy2/rpy2:devel",
      "SourceCode": "FROM rpy2/rpy2:devel",
      "Platform": "",
      "From": {
        "Image": "rpy2/rpy2:devel"
      },
      "Commands": [
        {
          "Maintainer": "Laurent Gautier \u003clgautier@gmail.com\u003e",
          "Name": "maintainer"
        },
        {
          "Name": "user",
          "User": "root"
        },
        {
          "CmdLine": [
            "apt-get update -qq \u0026\u0026   apt-get install -y                      ed                      git \t\t     libcairo-dev \t\t     libedit-dev                      lsb-release \t\t     llvm-3.8 \t\t     scala \t\t     wget \u0026\u0026  rm -rf /var/lib/apt/lists/*"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "wget --progress=bar http://mirrors.ocf.berkeley.edu/apache/spark/spark-2.1.0/spark-2.1.0-bin-hadoop2.7.tgz \u0026\u0026   tar -xzf spark-2.1.0-bin-hadoop2.7.tgz \u0026\u0026   mv spark-2.1.0-bin-hadoop2.7 /opt/ \u0026\u0026   rm spark-2.1.0-bin-hadoop2.7.tgz"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "pip3 --no-cache-dir install wheel --upgrade \u0026\u0026   pip3 --no-cache-dir install sqlalchemy \u0026\u0026   rm -rf /root/.cache \u0026\u0026   wget https://github.com/numba/llvmlite/archive/v0.15.0.zip \u0026\u0026   unzip v0.15.0.zip \u0026\u0026   cd llvmlite-0.15.0 \u0026\u0026   LLVM_CONFIG=`which llvm-config-3.8` python3 setup.py install \u0026\u0026   cd .. \u0026\u0026 rm -rf llvmlite-0.15.0 \u0026\u0026 rm v0.15.0.zip \u0026\u0026   pip3 --no-cache install numba \u0026\u0026   pip3 --no-cache install findspark \u0026\u0026   pip3 --no-cache install jupyter_dashboards \u0026\u0026   jupyter dashboards quick-setup --sys-prefix \u0026\u0026   rm -rf /root/.cache"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "echo \"glmnet\\n        gridExtra\\n        maps\\n        mapproj\\n        plotly\\n        RSQLite\\n        party\\n        partykit\\n        svglite\" \u003e rpacks.txt \u0026\u0026   R -e 'install.packages(sub(\"(.+)\\\\\\\\n\",\"\\\\1\", scan(\"rpacks.txt\", \"character\")), repos=\"http://cran.cnr.Berkeley.edu\")' \u0026\u0026   rm rpacks.txt"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "NB_USER",
              "Value": "jupyteruser"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_HOME",
              "Value": "/opt/spark-2.1.0-bin-hadoop2.7"
            }
          ],
          "Name": "env"
        },
        {
          "Name": "user",
          "User": "$NB_USER"
        },
        {
          "CmdLine": [
            "mkdir -p /home/$NB_USER/work"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Name": "workdir",
          "Path": "/home/$NB_USER/work"
        }
      ]
    }
  ]
}