MetaArgs: null
Stages:
- BaseName: hellgate75/apache-hadoop:2.8.0
  Commands:
  - Maintainer: Fabrizio Torelli (hellgate75@gmail.com)
    Name: maintainer
  - Env:
    - Key: SPARK_HOME
      Value: /usr/local/spark
    - Key: HADOOP_CONF_DIR
      Value: /usr/local/spark/etc/hadoop
    - Key: SPARK_VERSION
      Value: 2.1.1
    - Key: APACHE_HADOOP_VERSION
      Value: "2.7"
    - Key: HADOOP_LIBEXEC_DIR
      Value: $HADOOP_HOME/libexec
    - Key: SPARK_RUN_STANDALONE
      Value: "true"
    - Key: SPARK_HADOOP_TGZ_URL
      Value: '""'
    - Key: SPARK_CONFIG_TGZ_URL
      Value: '""'
    - Key: SPARK_START_HADOOP
      Value: "true"
    - Key: SPARK_START_HADOOP_ALL_SERVICES
      Value: "false"
    - Key: SPARK_START_HADOOP_HDFS
      Value: "true"
    - Key: SPARK_START_HADOOP_YARN
      Value: "false"
    - Key: SPARK_START_HADOOP_JOB_HISTORY
      Value: "false"
    - Key: SPARK_HADOOP_JOB_HISTORY_MAPRED_COMMAND
      Value: '""'
    - Key: SPARK_START_HADOOP_DEAMONS
      Value: "false"
    - Key: SPARK_START_HADOOP_DEAMON
      Value: "false"
    - Key: SPARK_HADOOP_DEAMON_COMMAND
      Value: '""'
    - Key: SPARK_START_HADOOP_YARN_DEAMONS
      Value: "false"
    - Key: SPARK_START_HADOOP_YARN_DEAMON
      Value: "false"
    - Key: SPARK_HADOOP_YARN_DEAMON_COMMAND
      Value: '""'
    - Key: SPARK_START_HADOOP_BALANCER
      Value: "false"
    - Key: SPARK_START_HADOOP_KMS_SERVER
      Value: "false"
    - Key: SPARK_START_MASTER_NODE
      Value: "true"
    - Key: SPARK_START_SLAVE_MASTER_HOST
      Value: '""'
    - Key: SPARK_START_SLAVE_MASTER_PORT
      Value: '""'
    - Key: SPARK_START_SLAVE_MASTER_WEBUI_PORT
      Value: '""'
    - Key: SPARK_START_SLAVE_CORES
      Value: "1"
    - Key: SPARK_START_SLAVE_MEMORY
      Value: 2g
    - Key: SPARK_START_SLAVE_WORKER_DIR
      Value: '"/usr/local/spark/work"'
    - Key: SPARK_START_ALL_SERVICES
      Value: "true"
    - Key: SPARK_START_ALL_SLAVES
      Value: "false"
    - Key: SPARK_START_HISTORY_SERVER
      Value: "false"
    - Key: SPARK_START_SHUFFLE_SERVICE
      Value: "false"
    - Key: SPARK_START_DEAMONS
      Value: "false"
    - Key: SPARK_START_DEAMON
      Value: "false"
    - Key: SPARK_DAEMON_COMMAND
      Value: '""'
    - Key: SPARK_START_THRIFT_SERVER
      Value: "false"
    - Key: SPARK_START_MESOS_INTEGRATION
      Value: "false"
    - Key: SPARK_START_INTEGRATE_WITH_YARN
      Value: "false"
    - Key: SPARK_START_YARN_CLASSNAME
      Value: '""'
    - Key: SPARK_START_YARN_ARGUMENTS
      Value: '""'
    - Key: SPARK_START_YARN_QUEUE_NAME
      Value: '"master-queue"'
    - Key: SPARK_FORCE_HADOOP_RESTART
      Value: "false"
    - Key: SPARK_FORCE_RESTART
      Value: "false"
    - Key: SPARK_CONFIG_LOG_EVENT_ENABLED
      Value: "true"
    - Key: SPARK_CONFIG_SERIALIZER_CLASS
      Value: org.apache.spark.serializer.KryoSerializer
    - Key: SPARK_CONFIG_DRIVER_MEMORY
      Value: 5g
    - Key: SPARK_CONFIG_EXTRA_JVM_OPTIONS
      Value: '""'
    - Key: SPARK_CONFIG_DEPLOY_RETAIN_APPS
      Value: "200"
    - Key: SPARK_CONFIG_DEPLOY_SPREADOUT
      Value: "true"
    - Key: SPARK_CONFIG_RETAIN_DRIVERS
      Value: "200"
    - Key: SPARK_CONFIG_DEPLOY_DEFAULT_CORES
      Value: "4"
    - Key: SPARK_CONFIG_DEPLOY_MAX_EXEC_RETRIES
      Value: "10"
    - Key: SPARK_CONFIG_WORKER_TIMEOUT
      Value: "60"
    - Key: SPARK_CONFIG_WORKER_CLEANUP_ENABLED
      Value: "false"
    - Key: SPARK_CONFIG_WORKER_CLEANUP_INTERVAL
      Value: "1800"
    - Key: SPARK_CONFIG_WORKER_CLEANUP_TTL
      Value: "604800"
    - Key: SPARK_CONFIG_WORKER_COMPRESSED_CACHE_SIZE
      Value: "100"
    - Key: SCALA_HOME
      Value: /usr/local/share/scala
    - Key: PATH
      Value: $PATH:/usr/local/share/scala/bin:/usr/local/bin:/usr/local/spark/bin:/usr/local/spark/sbin
    Name: env
  - Name: user
    User: root
  - CmdLine:
    - curl -s https://d3kbcqa49mib13.cloudfront.net/spark-$SPARK_VERSION-bin-hadoop$APACHE_HADOOP_VERSION.tgz
      | tar -xz -C /usr/local/
    Name: run
    PrependShell: true
  - CmdLine:
    - cd /usr/local && ln -s spark-$SPARK_VERSION-bin-hadoop$APACHE_HADOOP_VERSION
      spark
    Name: run
    PrependShell: true
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - bootstrap.sh
    - /etc/bootstrap.sh
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - docker-start-spark.sh
    - /usr/local/bin/docker-start-spark
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - spark-defaults-master.conf
    - /usr/local/spark/conf/spark-defaults-master.conf
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - spark-defaults-worker.conf
    - /usr/local/spark/conf/spark-defaults-worker.conf
  - CmdLine:
    - chown root.root /etc/bootstrap.sh &&     chown root.root /usr/local/bin/docker-start-spark
      &&     chmod +x /etc/bootstrap.sh &&     chmod +x /usr/local/bin/docker-start-spark
      &&     mkdir -p $HADOOP_CONF_DIR &&     mkdir -p /etc/config/spark &&     mkdir
      -p /root/application
    Name: run
    PrependShell: true
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - sources.list
    - /etc/apt/sources.backup.list
  - CmdLine:
    - cat /etc/apt/sources.list >> /etc/apt/sources.backup.list &&     cat /etc/apt/sources.backup.list
      > /etc/apt/sources.list
    Name: run
    PrependShell: true
  - CmdLine:
    - apt-get update &&     apt-get -y --no-install-recommends install apt-transport-https
      &&     apt-key adv --keyserver keyserver.ubuntu.com --recv-keys E298A3A825C0D65DFD57CBB651716619E084DAB9
      &&     add-apt-repository 'deb [arch=amd64,i386] https://cran.rstudio.com/bin/linux/ubuntu
      xenial/' &&     apt-get update &&     apt-get install -y r-base --no-install-recommends
      &&     apt-get clean &&     apt-get -y autoclean &&     rm -rf /var/lib/apt/lists/*
    Name: run
    PrependShell: true
  - Name: workdir
    Path: /root
  - CmdLine:
    - curl -LO http://www.scala-lang.org/files/archive/scala-2.12.2.deb &&     sudo
      dpkg -i ./scala-2.12.2.deb
    Name: run
    PrependShell: true
  - Name: workdir
    Path: $SPARK_HOME
  - CmdLine:
    - docker-start-spark
    - -daemon
    Name: cmd
    PrependShell: false
  - Name: volume
    Volumes:
    - /user/root/data/hadoop/hdfs/datanode
    - /user/root/data/hadoop/hdfs/namenode
    - /user/root/data/hadoop/hdfs/checkpoint
    - /etc/config/hadoop
    - /etc/config/spark
    - mkdir -p /root/application
  - Name: expose
    Ports:
    - "10020"
    - "19888"
    - "2122"
    - "49707"
    - "50010"
    - "50020"
    - "50070"
    - "50075"
    - "50090"
    - "8020"
    - "8030"
    - "8031"
    - "8032"
    - "8033"
    - "8040"
    - "8042"
    - "8080"
    - "8088"
    - "9000"
  From:
    Image: hellgate75/apache-hadoop:2.8.0
  Name: ""
  Platform: ""
  SourceCode: FROM hellgate75/apache-hadoop:2.8.0
