{
  "MetaArgs": null,
  "Stages": [
    {
      "Name": "",
      "BaseName": "fno2010/hadoop:2.7",
      "SourceCode": "FROM fno2010/hadoop:2.7",
      "Platform": "",
      "From": {
        "Image": "fno2010/hadoop:2.7"
      },
      "Commands": [
        {
          "Maintainer": "Jensen Zhang",
          "Name": "maintainer"
        },
        {
          "Env": [
            {
              "Key": "SPARK_VERSION",
              "Value": "2.1.1"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_HOME",
              "Value": "/usr/local/spark-$SPARK_VERSION"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "apt-get update   \u0026\u0026 DEBIAN_FRONTEND=noninteractive apt-get install     -yq --no-install-recommends        python python3 iputils-ping net-tools       openssh-client openssh-server   \u0026\u0026 apt-get clean \t\u0026\u0026 rm -rf /var/lib/apt/lists/*"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "mkdir -p \"${SPARK_HOME}\"   \u0026\u0026 export ARCHIVE=spark-$SPARK_VERSION-bin-without-hadoop.tgz   \u0026\u0026 export DOWNLOAD_PATH=apache/spark/spark-$SPARK_VERSION/$ARCHIVE   \u0026\u0026 curl -sSL https://mirrors.ocf.berkeley.edu/$DOWNLOAD_PATH |     tar -xz -C $SPARK_HOME --strip-components 1   \u0026\u0026 rm -rf $ARCHIVE"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "spark-env.sh",
            "$SPARK_HOME/conf/spark-env.sh"
          ]
        },
        {
          "Env": [
            {
              "Key": "PATH",
              "Value": "$PATH:$SPARK_HOME/bin"
            }
          ],
          "Name": "env"
        },
        {
          "Name": "expose",
          "Ports": [
            "6066",
            "7077",
            "8080",
            "8081"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "start-spark",
            "/opt/util/bin/start-spark"
          ]
        },
        {
          "CmdLine": [
            "sed -i 's/(hostname)/2/' /opt/util/bin/start-spark   \u0026\u0026 sed -i 's/namenode daemon/namenode daemon $2/' /opt/util/bin/start-spark   \u0026\u0026 sed -i 's/namenode \\$2/namenode $2 $3/' /opt/util/bin/start-hadoop   \u0026\u0026 sed -i 's/(hostname)/{2}/' /opt/util/bin/start-hadoop-namenode"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "echo \"export SPARK_HOME=$SPARK_HOME\" \u003e\u003e /etc/bash.bashrc   \u0026\u0026 echo 'export PATH=$PATH:$SPARK_HOME/bin'\u003e\u003e /etc/bash.bashrc"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "echo '#!/usr/bin/env bash' \u003e /usr/bin/master   \u0026\u0026 echo 'start-spark master $@' \u003e\u003e /usr/bin/master   \u0026\u0026 chmod +x /usr/bin/master   \u0026\u0026 echo '#!/usr/bin/env bash' \u003e /usr/bin/worker   \u0026\u0026 echo 'start-spark worker $@' \u003e\u003e /usr/bin/worker   \u0026\u0026 chmod +x /usr/bin/worker"
          ],
          "Name": "run",
          "PrependShell": true
        }
      ]
    }
  ]
}