{
  "MetaArgs": null,
  "Stages": [
    {
      "Name": "",
      "BaseName": "sequenceiq/hadoop-ubuntu:2.6.0",
      "SourceCode": "FROM sequenceiq/hadoop-ubuntu:2.6.0",
      "Platform": "",
      "From": {
        "Image": "sequenceiq/hadoop-ubuntu:2.6.0"
      },
      "Commands": [
        {
          "Maintainer": "\"Paolo D'Onorio De Meo \u003cp.donoriodemeo@gmail.com\u003e\"",
          "Name": "maintainer"
        },
        {
          "Env": [
            {
              "Key": "SPARK_VERSION",
              "Value": "spark-1.4.1"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "HADOOP_VERSION",
              "Value": "hadoop2.6"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_BIN",
              "Value": "\"$SPARK_VERSION-bin-$HADOOP_VERSION\""
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_URL",
              "Value": "\"http://www.eu.apache.org/dist/spark/$SPARK_VERSION/$SPARK_BIN.tgz\""
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "curl $SPARK_URL | tar -xz -C /usr/local/"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "cd /usr/local \u0026\u0026 ln -s $SPARK_BIN spark"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "SPARK_HOME",
              "Value": "/usr/local/spark"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "service ssh start \u0026\u0026     $HADOOP_PREFIX/etc/hadoop/hadoop-env.sh \u0026\u0026     $HADOOP_PREFIX/sbin/start-dfs.sh \u0026\u0026     $HADOOP_PREFIX/bin/hdfs dfsadmin -safemode leave \u0026\u0026     $HADOOP_PREFIX/bin/hdfs dfs -put $SPARK_HOME/lib /spark"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "SPARK_JAR",
              "Value": "hdfs:///spark/$SPARK_BIN.jar"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "PATH",
              "Value": "$PATH:$SPARK_HOME/bin:$HADOOP_PREFIX/bin"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "echo \"spark.master\\tspark://master:7077\"     \u003e $SPARK_HOME/conf/spark-defaults.conf"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "MYHOSTNAME",
              "Value": "master"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "sed \"s/HOSTNAME/$MYHOSTNAME/\"     $HADOOP_PREFIX/etc/hadoop/core-site.xml.template \u003e     $HADOOP_PREFIX/etc/hadoop/core-site.xml"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "BSMASTER",
              "Value": "/bootmaster.sh"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "echo \"#!/bin/bash\" \u003e $BSMASTER \u0026\u0026     echo \"spark-class org.apache.spark.deploy.master.Master \u0026\\nsleep 5\"         \u003e\u003e $BSMASTER \u0026\u0026     echo \"echo 'starting namenode'\" \u003e\u003e $BSMASTER \u0026\u0026     echo \"hdfs namenode \u003e /dev/null 2\u003e\u00261 \u0026\\nsleep 5\" \u003e\u003e $BSMASTER \u0026\u0026     echo \"echo 'starting secondary namenode'\" \u003e\u003e $BSMASTER \u0026\u0026     echo \"hdfs secondarynamenode \u003e /dev/null 2\u003e\u00261 \u0026\\nsleep 5\" \u003e\u003e $BSMASTER \u0026\u0026     echo \"echo 'starting datanode'\" \u003e\u003e $BSMASTER \u0026\u0026     echo \"hdfs dfsadmin -safemode leave \u0026\u0026 hdfs datanode \u003e /dev/null 2\u003e\u00261\"         \u003e\u003e $BSMASTER"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Env": [
            {
              "Key": "BSWORKER",
              "Value": "/bootslave.sh"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "echo \"#!/bin/bash\" \u003e $BSWORKER \u0026\u0026     echo \"spark-class org.apache.spark.deploy.worker.Worker spark://master:7077\" \u003e\u003e $BSWORKER"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "chmod +x $BSWORKER $BSMASTER"
          ],
          "Name": "run",
          "PrependShell": true
        }
      ]
    }
  ]
}