MetaArgs: null
Stages:
- BaseName: python:3.7.4-buster
  Commands:
  - Key: SPARK_JARS
    Name: arg
    Value: jars
  - Key: IMG_PATH
    Name: arg
    Value: kubernetes/dockerfiles
  - Key: SPARK_VERSION
    Name: arg
    Value: 2.4.0
  - Env:
    - Key: SPARK_HOME
      Value: /opt/spark
    Name: env
  - Env:
    - Key: JAVA_HOME
      Value: /usr/lib/jvm/adoptopenjdk-8-hotspot-amd64
    Name: env
  - Env:
    - Key: PYTHONPATH
      Value: ${SPARK_HOME}/python/lib/pyspark.zip:${SPARK_HOME}/python/lib/py4j-*.zip
    Name: env
  - Env:
    - Key: PATH
      Value: ${PATH}:${SPARK_HOME}/bin
    Name: env
  - Name: workdir
    Path: /
  - CmdLine:
    - apt-get update &&     wget -qO - https://adoptopenjdk.jfrog.io/adoptopenjdk/api/gpg/key/public
      | apt-key add - &&     apt-get install -y software-properties-common &&     add-apt-repository
      --yes https://adoptopenjdk.jfrog.io/adoptopenjdk/deb/ &&     apt-get update
      &&     apt-get install -y adoptopenjdk-8-hotspot python3-dev &&     curl https://archive.apache.org/dist/spark/spark-${SPARK_VERSION}/spark-${SPARK_VERSION}-bin-hadoop2.7.tgz
      -OLJ &&     tar -xzvf spark-${SPARK_VERSION}-bin-hadoop2.7.tgz &&     cd spark-${SPARK_VERSION}-bin-hadoop2.7
      &&     mkdir -p ${SPARK_HOME}/python &&     set -ex &&     mkdir -p /opt/spark
      &&     mkdir -p /opt/spark/work-dir &&     touch /opt/spark/RELEASE &&     rm
      /bin/sh &&     ln -sv /bin/bash /bin/sh &&     echo "auth required pam_wheel.so
      use_uid" >> /etc/pam.d/su &&     chgrp root /etc/passwd && chmod ug+rw /etc/passwd
      &&     cp -r ${SPARK_JARS} /opt/spark/jars &&     cp -r bin /opt/spark/bin &&     cp
      -r sbin /opt/spark/sbin &&     cp -r ${IMG_PATH}/spark/entrypoint.sh /opt/ &&     cp
      -r examples /opt/spark/examples &&     cp -r data /opt/spark/data &&     cp
      -r python/lib ${SPARK_HOME}/python/lib &&     sed -i -e 's/\/sbin\/tini[^"]*//g'
      /opt/entrypoint.sh &&     rm -rf /spark-${SPARK_VERSION}-bin-hadoop2.7
    Name: run
    PrependShell: true
  - CmdLine:
    - cd ${SPARK_HOME}/jars &&     curl https://storage.googleapis.com/hadoop-lib/gcs/gcs-connector-hadoop2-latest.jar
      -OLJ &&     curl https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/2.7.3/hadoop-aws-2.7.3.jar
      -OLJ &&     curl https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk/1.7.4/aws-java-sdk-1.7.4.jar
      -OLJ &&     rm kubernetes-client-3.0.0.jar &&     curl https://repo1.maven.org/maven2/io/fabric8/kubernetes-client/4.4.2/kubernetes-client-4.4.2.jar
      -OLJ
    Name: run
    PrependShell: true
  - Name: workdir
    Path: /opt/spark/work-dir
  - CmdLine:
    - /opt/entrypoint.sh
    Name: entrypoint
    PrependShell: false
  From:
    Image: python:3.7.4-buster
  Name: ""
  Platform: ""
  SourceCode: FROM python:3.7.4-buster
