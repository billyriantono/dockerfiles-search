{
  "MetaArgs": null,
  "Stages": [
    {
      "Name": "",
      "BaseName": "continuumio/miniconda3:latest",
      "SourceCode": "FROM continuumio/miniconda3:latest",
      "Platform": "",
      "From": {
        "Image": "continuumio/miniconda3:latest"
      },
      "Commands": [
        {
          "Env": [
            {
              "Key": "HADOOP_HOME",
              "Value": "/opt/hadoop"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "SPARK_HOME",
              "Value": "/opt/spark"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "LIVY_HOME",
              "Value": "/opt/livy"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "JAVA_HOME",
              "Value": "/usr/lib/jvm/java-8-openjdk-amd64"
            }
          ],
          "Name": "env"
        },
        {
          "Env": [
            {
              "Key": "HOME",
              "Value": "/root"
            }
          ],
          "Name": "env"
        },
        {
          "CmdLine": [
            "apt-get update \u0026\u0026 apt-get install -y   openssh-server   rsync   vim   sudo   zip   openjdk-8-jdk --fix-missing -y"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "wget http://mirrors.estointernet.in/apache/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz \u0026\u0026   wget http://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz \u0026\u0026   wget http://archive.apache.org/dist/incubator/livy/0.5.0-incubating/livy-0.5.0-incubating-bin.zip \u0026\u0026   tar xzf hadoop-2.8.4.tar.gz \u0026\u0026   tar xzf spark-2.4.2-bin-hadoop2.7.tgz \u0026\u0026   unzip livy-0.5.0-incubating-bin.zip \u0026\u0026   mv hadoop-2.8.4 $HADOOP_HOME \u0026\u0026   mv spark-2.4.2-bin-hadoop2.7 $SPARK_HOME \u0026\u0026   mv livy-0.5.0-incubating-bin $LIVY_HOME \u0026\u0026   rm -rf hadoop-2.8.4.tar.gz spark-2.4.2-bin-hadoop2.7.tgz livy-0.5.0-incubating-bin.zip \u0026\u0026   mkdir -p /opt/hadoop/data/hdfs/namenode \u0026\u0026   mkdir -p /opt/hadoop/data/hdfs/datanode"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa \u0026\u0026   cat ~/.ssh/id_rsa.pub \u003e\u003e ~/.ssh/authorized_keys \u0026\u0026   chmod 0600 ~/.ssh/authorized_keys"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/*xml",
            "$HADOOP_HOME/etc/hadoop/"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/spark-defaults.conf",
            "$SPARK_HOME/conf/"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/ssh_config",
            "/root/.ssh/config"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/env.sh",
            "/tmp/env.sh"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "script/start-hadoop.sh",
            "."
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/requirements.txt",
            "."
          ]
        },
        {
          "CmdLine": [
            "pip install jupyter"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "pip install -r requirements.txt"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "mkdir -p ${HOME}/notebooks"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Name": "workdir",
          "Path": "${HOME}"
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "libs/*.zip",
            "."
          ]
        },
        {
          "CmdLine": [
            "unzip s3libs.zip \u0026\u0026 rm -f s3libs.zip"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "jupyter notebook --generate-config"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "ipython profile create pyspark"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "mkdir -p .ipython/kernels/pyspark"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/00-default-setup.py",
            ".ipython/profile_pyspark/startup/"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/00-default-setup.py",
            ".ipython/profile_default/startup/"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/kernel.json",
            ".ipython/kernels/pyspark"
          ]
        },
        {
          "Chown": "",
          "From": "",
          "Name": "copy",
          "SourcesAndDest": [
            "config/jupyter_notebook_config.py",
            ".jupyter/"
          ]
        },
        {
          "CmdLine": [
            "/tmp/env.sh \u0026\u0026 rm -f /tmp/env.sh"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "CmdLine": [
            "chmod +x /start-hadoop.sh"
          ],
          "Name": "run",
          "PrependShell": true
        },
        {
          "Name": "expose",
          "Ports": [
            "2222",
            "4040",
            "50070",
            "50075",
            "8020",
            "8030",
            "8031",
            "8032",
            "8033",
            "8042",
            "8088",
            "8888",
            "8998"
          ]
        },
        {
          "CmdLine": [
            "sh",
            "-c",
            "/start-hadoop.sh"
          ],
          "Name": "cmd",
          "PrependShell": false
        }
      ]
    }
  ]
}