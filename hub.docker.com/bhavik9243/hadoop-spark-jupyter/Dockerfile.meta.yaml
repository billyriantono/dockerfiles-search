MetaArgs: null
Stages:
- BaseName: continuumio/miniconda3:latest
  Commands:
  - Env:
    - Key: HADOOP_HOME
      Value: /opt/hadoop
    Name: env
  - Env:
    - Key: SPARK_HOME
      Value: /opt/spark
    Name: env
  - Env:
    - Key: LIVY_HOME
      Value: /opt/livy
    Name: env
  - Env:
    - Key: JAVA_HOME
      Value: /usr/lib/jvm/java-8-openjdk-amd64
    Name: env
  - Env:
    - Key: HOME
      Value: /root
    Name: env
  - CmdLine:
    - apt-get update && apt-get install -y   openssh-server   rsync   vim   sudo   zip   openjdk-8-jdk
      --fix-missing -y
    Name: run
    PrependShell: true
  - CmdLine:
    - wget http://mirrors.estointernet.in/apache/hadoop/common/hadoop-2.8.4/hadoop-2.8.4.tar.gz
      &&   wget http://archive.apache.org/dist/spark/spark-2.4.2/spark-2.4.2-bin-hadoop2.7.tgz
      &&   wget http://archive.apache.org/dist/incubator/livy/0.5.0-incubating/livy-0.5.0-incubating-bin.zip
      &&   tar xzf hadoop-2.8.4.tar.gz &&   tar xzf spark-2.4.2-bin-hadoop2.7.tgz
      &&   unzip livy-0.5.0-incubating-bin.zip &&   mv hadoop-2.8.4 $HADOOP_HOME &&   mv
      spark-2.4.2-bin-hadoop2.7 $SPARK_HOME &&   mv livy-0.5.0-incubating-bin $LIVY_HOME
      &&   rm -rf hadoop-2.8.4.tar.gz spark-2.4.2-bin-hadoop2.7.tgz livy-0.5.0-incubating-bin.zip
      &&   mkdir -p /opt/hadoop/data/hdfs/namenode &&   mkdir -p /opt/hadoop/data/hdfs/datanode
    Name: run
    PrependShell: true
  - CmdLine:
    - ssh-keygen -t rsa -P '' -f ~/.ssh/id_rsa &&   cat ~/.ssh/id_rsa.pub >> ~/.ssh/authorized_keys
      &&   chmod 0600 ~/.ssh/authorized_keys
    Name: run
    PrependShell: true
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/*xml
    - $HADOOP_HOME/etc/hadoop/
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/spark-defaults.conf
    - $SPARK_HOME/conf/
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/ssh_config
    - /root/.ssh/config
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/env.sh
    - /tmp/env.sh
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - script/start-hadoop.sh
    - .
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/requirements.txt
    - .
  - CmdLine:
    - pip install jupyter
    Name: run
    PrependShell: true
  - CmdLine:
    - pip install -r requirements.txt
    Name: run
    PrependShell: true
  - CmdLine:
    - mkdir -p ${HOME}/notebooks
    Name: run
    PrependShell: true
  - Name: workdir
    Path: ${HOME}
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - libs/*.zip
    - .
  - CmdLine:
    - unzip s3libs.zip && rm -f s3libs.zip
    Name: run
    PrependShell: true
  - CmdLine:
    - jupyter notebook --generate-config
    Name: run
    PrependShell: true
  - CmdLine:
    - ipython profile create pyspark
    Name: run
    PrependShell: true
  - CmdLine:
    - mkdir -p .ipython/kernels/pyspark
    Name: run
    PrependShell: true
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/00-default-setup.py
    - .ipython/profile_pyspark/startup/
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/00-default-setup.py
    - .ipython/profile_default/startup/
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/kernel.json
    - .ipython/kernels/pyspark
  - Chown: ""
    From: ""
    Name: copy
    SourcesAndDest:
    - config/jupyter_notebook_config.py
    - .jupyter/
  - CmdLine:
    - /tmp/env.sh && rm -f /tmp/env.sh
    Name: run
    PrependShell: true
  - CmdLine:
    - chmod +x /start-hadoop.sh
    Name: run
    PrependShell: true
  - Name: expose
    Ports:
    - "2222"
    - "4040"
    - "50070"
    - "50075"
    - "8020"
    - "8030"
    - "8031"
    - "8032"
    - "8033"
    - "8042"
    - "8088"
    - "8888"
    - "8998"
  - CmdLine:
    - sh
    - -c
    - /start-hadoop.sh
    Name: cmd
    PrependShell: false
  From:
    Image: continuumio/miniconda3:latest
  Name: ""
  Platform: ""
  SourceCode: FROM continuumio/miniconda3:latest
